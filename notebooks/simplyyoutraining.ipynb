{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0RlzseFJ2m-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ed3c841"
      },
      "source": [
        "# Task\n",
        "Fine-tune ImageNet-pretrained MobileNetV2 for multi-class image classification on Windows using PyTorch. Use the dataset at \"C:\\Windows\\System32\\curated_indian_food_dataset\". Perform data verification, create train/val/test splits, set up dataloaders with augmentations, define and modify the MobileNetV2 model, train the model in two phases (head-only and fine-tuning), test the best model, export the model in .pth and .onnx formats, create an inference script, and save all artifacts under `./artifacts` with a README. Ensure the code is Windows-friendly, handles OOM by reducing batch size, skips corrupt images, and includes necessary checks and assertions with clear error messages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4523ad70"
      },
      "source": [
        "## Setup\n",
        "\n",
        "### Subtask:\n",
        "Install necessary libraries and define constants.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d619ca5e"
      },
      "source": [
        "**Reasoning**:\n",
        "Install the required Python packages using pip.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36b5738f",
        "outputId": "ff90c647-f102-4361-dc9b-3667c7c567b1"
      },
      "source": [
        "%pip install torch torchvision numpy matplotlib scikit-learn onnx onnxruntime Pillow"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in c:\\programdata\\miniconda3\\envs\\hack1\\lib\\site-packages (2.5.1)\n",
            "Requirement already satisfied: torchvision in c:\\programdata\\miniconda3\\envs\\hack1\\lib\\site-packages (0.20.1)\n",
            "Requirement already satisfied: numpy in c:\\programdata\\miniconda3\\envs\\hack1\\lib\\site-packages (2.0.1)\n",
            "Collecting matplotlib\n",
            "  Downloading matplotlib-3.10.7-cp310-cp310-win_amd64.whl.metadata (11 kB)\n",
            "Collecting scikit-learn\n",
            "  Using cached scikit_learn-1.7.2-cp310-cp310-win_amd64.whl.metadata (11 kB)\n",
            "Collecting onnx\n",
            "  Downloading onnx-1.19.1-cp310-cp310-win_amd64.whl.metadata (7.2 kB)\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.23.1-cp310-cp310-win_amd64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: Pillow in c:\\programdata\\miniconda3\\envs\\hack1\\lib\\site-packages (11.1.0)\n",
            "Requirement already satisfied: filelock in c:\\programdata\\miniconda3\\envs\\hack1\\lib\\site-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\programdata\\miniconda3\\envs\\hack1\\lib\\site-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: networkx in c:\\programdata\\miniconda3\\envs\\hack1\\lib\\site-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in c:\\programdata\\miniconda3\\envs\\hack1\\lib\\site-packages (from torch) (3.1.6)\n",
            "Collecting fsspec (from torch)\n",
            "  Using cached fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting sympy==1.13.1 (from torch)\n",
            "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\miniconda3\\envs\\hack1\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib)\n",
            "  Using cached contourpy-1.3.2-cp310-cp310-win_amd64.whl.metadata (5.5 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib)\n",
            "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib)\n",
            "  Using cached fonttools-4.60.1-cp310-cp310-win_amd64.whl.metadata (114 kB)\n",
            "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
            "  Using cached kiwisolver-1.4.9-cp310-cp310-win_amd64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\miniconda3\\envs\\hack1\\lib\\site-packages (from matplotlib) (25.0)\n",
            "Collecting pyparsing>=3 (from matplotlib)\n",
            "  Using cached pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\programdata\\miniconda3\\envs\\hack1\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
            "Collecting scipy>=1.8.0 (from scikit-learn)\n",
            "  Using cached scipy-1.15.3-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
            "Collecting joblib>=1.2.0 (from scikit-learn)\n",
            "  Using cached joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
            "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting protobuf>=4.25.1 (from onnx)\n",
            "  Using cached protobuf-6.32.1-cp310-abi3-win_amd64.whl.metadata (593 bytes)\n",
            "Collecting ml_dtypes>=0.5.0 (from onnx)\n",
            "  Downloading ml_dtypes-0.5.3-cp310-cp310-win_amd64.whl.metadata (9.2 kB)\n",
            "Collecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Collecting flatbuffers (from onnxruntime)\n",
            "  Downloading flatbuffers-25.9.23-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Requirement already satisfied: six>=1.5 in c:\\programdata\\miniconda3\\envs\\hack1\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting pyreadline3 (from humanfriendly>=9.1->coloredlogs->onnxruntime)\n",
            "  Downloading pyreadline3-3.5.4-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\miniconda3\\envs\\hack1\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
            "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "Downloading matplotlib-3.10.7-cp310-cp310-win_amd64.whl (8.1 MB)\n",
            "   ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
            "   -- ------------------------------------- 0.5/8.1 MB 5.6 MB/s eta 0:00:02\n",
            "   ------- -------------------------------- 1.6/8.1 MB 4.7 MB/s eta 0:00:02\n",
            "   --------------- ------------------------ 3.1/8.1 MB 5.8 MB/s eta 0:00:01\n",
            "   --------------------- ------------------ 4.5/8.1 MB 6.2 MB/s eta 0:00:01\n",
            "   ------------------------- -------------- 5.2/8.1 MB 5.6 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 6.8/8.1 MB 6.0 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 8.1/8.1 MB 6.1 MB/s  0:00:01\n",
            "Using cached scikit_learn-1.7.2-cp310-cp310-win_amd64.whl (8.9 MB)\n",
            "Downloading onnx-1.19.1-cp310-cp310-win_amd64.whl (16.5 MB)\n",
            "   ---------------------------------------- 0.0/16.5 MB ? eta -:--:--\n",
            "   -- ------------------------------------- 1.0/16.5 MB 5.6 MB/s eta 0:00:03\n",
            "   ----- ---------------------------------- 2.1/16.5 MB 5.1 MB/s eta 0:00:03\n",
            "   ------- -------------------------------- 2.9/16.5 MB 4.7 MB/s eta 0:00:03\n",
            "   ---------- ----------------------------- 4.2/16.5 MB 5.1 MB/s eta 0:00:03\n",
            "   ---------- ----------------------------- 4.5/16.5 MB 4.6 MB/s eta 0:00:03\n",
            "   ------------ --------------------------- 5.2/16.5 MB 4.2 MB/s eta 0:00:03\n",
            "   -------------- ------------------------- 6.0/16.5 MB 4.3 MB/s eta 0:00:03\n",
            "   ----------------- ---------------------- 7.1/16.5 MB 4.4 MB/s eta 0:00:03\n",
            "   ------------------ --------------------- 7.6/16.5 MB 4.3 MB/s eta 0:00:03\n",
            "   --------------------- ------------------ 8.9/16.5 MB 4.3 MB/s eta 0:00:02\n",
            "   ---------------------- ----------------- 9.4/16.5 MB 4.4 MB/s eta 0:00:02\n",
            "   ------------------------- -------------- 10.5/16.5 MB 4.2 MB/s eta 0:00:02\n",
            "   --------------------------- ------------ 11.3/16.5 MB 4.2 MB/s eta 0:00:02\n",
            "   ----------------------------- ---------- 12.1/16.5 MB 4.2 MB/s eta 0:00:02\n",
            "   ------------------------------- -------- 13.1/16.5 MB 4.2 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 14.4/16.5 MB 4.4 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 14.7/16.5 MB 4.3 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 15.7/16.5 MB 4.3 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 16.0/16.5 MB 4.1 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 16.5/16.5 MB 4.1 MB/s  0:00:04\n",
            "Downloading onnxruntime-1.23.1-cp310-cp310-win_amd64.whl (13.5 MB)\n",
            "   ---------------------------------------- 0.0/13.5 MB ? eta -:--:--\n",
            "   --- ------------------------------------ 1.3/13.5 MB 6.1 MB/s eta 0:00:02\n",
            "   ------ --------------------------------- 2.1/13.5 MB 6.2 MB/s eta 0:00:02\n",
            "   ------- -------------------------------- 2.6/13.5 MB 4.6 MB/s eta 0:00:03\n",
            "   ---------- ----------------------------- 3.7/13.5 MB 4.5 MB/s eta 0:00:03\n",
            "   -------------- ------------------------- 4.7/13.5 MB 4.6 MB/s eta 0:00:02\n",
            "   ------------------ --------------------- 6.3/13.5 MB 5.1 MB/s eta 0:00:02\n",
            "   -------------------- ------------------- 6.8/13.5 MB 5.2 MB/s eta 0:00:02\n",
            "   ---------------------- ----------------- 7.6/13.5 MB 4.6 MB/s eta 0:00:02\n",
            "   ------------------------ --------------- 8.4/13.5 MB 4.6 MB/s eta 0:00:02\n",
            "   ------------------------ --------------- 8.4/13.5 MB 4.6 MB/s eta 0:00:02\n",
            "   ----------------------------- ---------- 10.0/13.5 MB 4.4 MB/s eta 0:00:01\n",
            "   ------------------------------- -------- 10.5/13.5 MB 4.5 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 11.3/13.5 MB 4.3 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 12.3/13.5 MB 4.3 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 13.1/13.5 MB 4.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 13.5/13.5 MB 4.2 MB/s  0:00:03\n",
            "Using cached contourpy-1.3.2-cp310-cp310-win_amd64.whl (221 kB)\n",
            "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Using cached fonttools-4.60.1-cp310-cp310-win_amd64.whl (2.3 MB)\n",
            "Using cached joblib-1.5.2-py3-none-any.whl (308 kB)\n",
            "Using cached kiwisolver-1.4.9-cp310-cp310-win_amd64.whl (73 kB)\n",
            "Downloading ml_dtypes-0.5.3-cp310-cp310-win_amd64.whl (206 kB)\n",
            "Using cached protobuf-6.32.1-cp310-abi3-win_amd64.whl (435 kB)\n",
            "Using cached pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
            "Using cached scipy-1.15.3-cp310-cp310-win_amd64.whl (41.3 MB)\n",
            "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
            "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "Downloading flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
            "Using cached fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
            "Downloading pyreadline3-3.5.4-py3-none-any.whl (83 kB)\n",
            "Installing collected packages: flatbuffers, threadpoolctl, sympy, scipy, pyreadline3, pyparsing, protobuf, ml_dtypes, kiwisolver, joblib, fsspec, fonttools, cycler, contourpy, scikit-learn, onnx, matplotlib, humanfriendly, coloredlogs, onnxruntime\n",
            "\n",
            "  Attempting uninstall: sympy\n",
            "\n",
            "    Found existing installation: sympy 1.14.0\n",
            "\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "    Uninstalling sympy-1.14.0:\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "      Successfully uninstalled sympy-1.14.0\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "   ---- -----------------------------------  2/20 [sympy]\n",
            "   ------ ---------------------------------  3/20 [scipy]\n",
            "   ------ ---------------------------------  3/20 [scipy]\n",
            "   ------ ---------------------------------  3/20 [scipy]\n",
            "   ------ ---------------------------------  3/20 [scipy]\n",
            "   ------ ---------------------------------  3/20 [scipy]\n",
            "   ------ ---------------------------------  3/20 [scipy]\n",
            "   ------ ---------------------------------  3/20 [scipy]\n",
            "   ------ ---------------------------------  3/20 [scipy]\n",
            "   ------ ---------------------------------  3/20 [scipy]\n",
            "   ------ ---------------------------------  3/20 [scipy]\n",
            "   ------ ---------------------------------  3/20 [scipy]\n",
            "   ------ ---------------------------------  3/20 [scipy]\n",
            "   ------ ---------------------------------  3/20 [scipy]\n",
            "   ------ ---------------------------------  3/20 [scipy]\n",
            "   ------ ---------------------------------  3/20 [scipy]\n",
            "   ------ ---------------------------------  3/20 [scipy]\n",
            "   ------ ---------------------------------  3/20 [scipy]\n",
            "   ------ ---------------------------------  3/20 [scipy]\n",
            "   ------ ---------------------------------  3/20 [scipy]\n",
            "   ------ ---------------------------------  3/20 [scipy]\n",
            "   ------ ---------------------------------  3/20 [scipy]\n",
            "   ------ ---------------------------------  3/20 [scipy]\n",
            "   ------ ---------------------------------  3/20 [scipy]\n",
            "   ------ ---------------------------------  3/20 [scipy]\n",
            "   ------ ---------------------------------  3/20 [scipy]\n",
            "   ------ ---------------------------------  3/20 [scipy]\n",
            "   ------ ---------------------------------  3/20 [scipy]\n",
            "   ------ ---------------------------------  3/20 [scipy]\n",
            "   ------ ---------------------------------  3/20 [scipy]\n",
            "   ------ ---------------------------------  3/20 [scipy]\n",
            "   ------ ---------------------------------  3/20 [scipy]\n",
            "   ------ ---------------------------------  3/20 [scipy]\n",
            "   ------ ---------------------------------  3/20 [scipy]\n",
            "   ------ ---------------------------------  3/20 [scipy]\n",
            "   ------ ---------------------------------  3/20 [scipy]\n",
            "   ------ ---------------------------------  3/20 [scipy]\n",
            "   ---------- -----------------------------  5/20 [pyparsing]\n",
            "   ------------ ---------------------------  6/20 [protobuf]\n",
            "   ------------------ ---------------------  9/20 [joblib]\n",
            "   ------------------ ---------------------  9/20 [joblib]\n",
            "   ------------------ ---------------------  9/20 [joblib]\n",
            "   -------------------- ------------------- 10/20 [fsspec]\n",
            "   ---------------------- ----------------- 11/20 [fonttools]\n",
            "   ---------------------- ----------------- 11/20 [fonttools]\n",
            "   ---------------------- ----------------- 11/20 [fonttools]\n",
            "   ---------------------- ----------------- 11/20 [fonttools]\n",
            "   ---------------------- ----------------- 11/20 [fonttools]\n",
            "   ---------------------- ----------------- 11/20 [fonttools]\n",
            "   ---------------------- ----------------- 11/20 [fonttools]\n",
            "   ---------------------- ----------------- 11/20 [fonttools]\n",
            "   ---------------------- ----------------- 11/20 [fonttools]\n",
            "   ---------------------------- ----------- 14/20 [scikit-learn]\n",
            "   ---------------------------- ----------- 14/20 [scikit-learn]\n",
            "   ---------------------------- ----------- 14/20 [scikit-learn]\n",
            "   ---------------------------- ----------- 14/20 [scikit-learn]\n",
            "   ---------------------------- ----------- 14/20 [scikit-learn]\n",
            "   ---------------------------- ----------- 14/20 [scikit-learn]\n",
            "   ---------------------------- ----------- 14/20 [scikit-learn]\n",
            "   ---------------------------- ----------- 14/20 [scikit-learn]\n",
            "   ---------------------------- ----------- 14/20 [scikit-learn]\n",
            "   ---------------------------- ----------- 14/20 [scikit-learn]\n",
            "   ---------------------------- ----------- 14/20 [scikit-learn]\n",
            "   ---------------------------- ----------- 14/20 [scikit-learn]\n",
            "   ---------------------------- ----------- 14/20 [scikit-learn]\n",
            "   ---------------------------- ----------- 14/20 [scikit-learn]\n",
            "   ---------------------------- ----------- 14/20 [scikit-learn]\n",
            "   ---------------------------- ----------- 14/20 [scikit-learn]\n",
            "   ---------------------------- ----------- 14/20 [scikit-learn]\n",
            "   ---------------------------- ----------- 14/20 [scikit-learn]\n",
            "   ---------------------------- ----------- 14/20 [scikit-learn]\n",
            "   ---------------------------- ----------- 14/20 [scikit-learn]\n",
            "   ---------------------------- ----------- 14/20 [scikit-learn]\n",
            "   ------------------------------ --------- 15/20 [onnx]\n",
            "   ------------------------------ --------- 15/20 [onnx]\n",
            "   ------------------------------ --------- 15/20 [onnx]\n",
            "   ------------------------------ --------- 15/20 [onnx]\n",
            "   ------------------------------ --------- 15/20 [onnx]\n",
            "   ------------------------------ --------- 15/20 [onnx]\n",
            "   ------------------------------ --------- 15/20 [onnx]\n",
            "   ------------------------------ --------- 15/20 [onnx]\n",
            "   ------------------------------ --------- 15/20 [onnx]\n",
            "   ------------------------------ --------- 15/20 [onnx]\n",
            "   ------------------------------ --------- 15/20 [onnx]\n",
            "   ------------------------------ --------- 15/20 [onnx]\n",
            "   ------------------------------ --------- 15/20 [onnx]\n",
            "   ------------------------------ --------- 15/20 [onnx]\n",
            "   ------------------------------ --------- 15/20 [onnx]\n",
            "   ------------------------------ --------- 15/20 [onnx]\n",
            "   ------------------------------ --------- 15/20 [onnx]\n",
            "   ------------------------------ --------- 15/20 [onnx]\n",
            "   ------------------------------ --------- 15/20 [onnx]\n",
            "   ------------------------------ --------- 15/20 [onnx]\n",
            "   ------------------------------ --------- 15/20 [onnx]\n",
            "   ------------------------------ --------- 15/20 [onnx]\n",
            "   ------------------------------ --------- 15/20 [onnx]\n",
            "   ------------------------------ --------- 15/20 [onnx]\n",
            "   ------------------------------ --------- 15/20 [onnx]\n",
            "   ------------------------------ --------- 15/20 [onnx]\n",
            "   ------------------------------ --------- 15/20 [onnx]\n",
            "   ------------------------------ --------- 15/20 [onnx]\n",
            "   ------------------------------ --------- 15/20 [onnx]\n",
            "   ------------------------------ --------- 15/20 [onnx]\n",
            "   ------------------------------ --------- 15/20 [onnx]\n",
            "   ------------------------------ --------- 15/20 [onnx]\n",
            "   ------------------------------ --------- 15/20 [onnx]\n",
            "   ------------------------------ --------- 15/20 [onnx]\n",
            "   ------------------------------ --------- 15/20 [onnx]\n",
            "   ------------------------------ --------- 15/20 [onnx]\n",
            "   ------------------------------ --------- 15/20 [onnx]\n",
            "   ------------------------------ --------- 15/20 [onnx]\n",
            "   ------------------------------ --------- 15/20 [onnx]\n",
            "   ------------------------------ --------- 15/20 [onnx]\n",
            "   ------------------------------ --------- 15/20 [onnx]\n",
            "   ------------------------------ --------- 15/20 [onnx]\n",
            "   ------------------------------ --------- 15/20 [onnx]\n",
            "   ------------------------------ --------- 15/20 [onnx]\n",
            "   ------------------------------ --------- 15/20 [onnx]\n",
            "   ------------------------------ --------- 15/20 [onnx]\n",
            "   ------------------------------ --------- 15/20 [onnx]\n",
            "   ------------------------------ --------- 15/20 [onnx]\n",
            "   ------------------------------ --------- 15/20 [onnx]\n",
            "   ------------------------------ --------- 15/20 [onnx]\n",
            "   ------------------------------ --------- 15/20 [onnx]\n",
            "   ------------------------------ --------- 15/20 [onnx]\n",
            "   ------------------------------ --------- 15/20 [onnx]\n",
            "   ------------------------------ --------- 15/20 [onnx]\n",
            "   ------------------------------ --------- 15/20 [onnx]\n",
            "   ------------------------------ --------- 15/20 [onnx]\n",
            "   ------------------------------ --------- 15/20 [onnx]\n",
            "   -------------------------------- ------- 16/20 [matplotlib]\n",
            "   -------------------------------- ------- 16/20 [matplotlib]\n",
            "   -------------------------------- ------- 16/20 [matplotlib]\n",
            "   -------------------------------- ------- 16/20 [matplotlib]\n",
            "   -------------------------------- ------- 16/20 [matplotlib]\n",
            "   -------------------------------- ------- 16/20 [matplotlib]\n",
            "   -------------------------------- ------- 16/20 [matplotlib]\n",
            "   -------------------------------- ------- 16/20 [matplotlib]\n",
            "   -------------------------------- ------- 16/20 [matplotlib]\n",
            "   -------------------------------- ------- 16/20 [matplotlib]\n",
            "   -------------------------------- ------- 16/20 [matplotlib]\n",
            "   ---------------------------------- ----- 17/20 [humanfriendly]\n",
            "   -------------------------------------- - 19/20 [onnxruntime]\n",
            "   -------------------------------------- - 19/20 [onnxruntime]\n",
            "   -------------------------------------- - 19/20 [onnxruntime]\n",
            "   -------------------------------------- - 19/20 [onnxruntime]\n",
            "   -------------------------------------- - 19/20 [onnxruntime]\n",
            "   -------------------------------------- - 19/20 [onnxruntime]\n",
            "   -------------------------------------- - 19/20 [onnxruntime]\n",
            "   ---------------------------------------- 20/20 [onnxruntime]\n",
            "\n",
            "Successfully installed coloredlogs-15.0.1 contourpy-1.3.2 cycler-0.12.1 flatbuffers-25.9.23 fonttools-4.60.1 fsspec-2025.9.0 humanfriendly-10.0 joblib-1.5.2 kiwisolver-1.4.9 matplotlib-3.10.7 ml_dtypes-0.5.3 onnx-1.19.1 onnxruntime-1.23.1 protobuf-6.32.1 pyparsing-3.2.5 pyreadline3-3.5.4 scikit-learn-1.7.2 scipy-1.15.3 sympy-1.13.1 threadpoolctl-3.6.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2847517"
      },
      "source": [
        "**Reasoning**:\n",
        "Define constants for the dataset path, image dimensions, batch size, learning rates, number of epochs, and artifact paths.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae4b1ed8",
        "outputId": "e04b8370-fc7a-476e-a2f8-691cc467b087"
      },
      "source": [
        "import os\n",
        "\n",
        "# Define dataset path\n",
        "DATA_DIR = \"C:\\\\Windows\\\\System32\\\\curated_indian_food_dataset\"\n",
        "if not os.path.exists(DATA_DIR):\n",
        "    print(f\"Error: Dataset path not found at {DATA_DIR}. Please verify the path.\")\n",
        "    # Although the instructions say not to ask for help,\n",
        "    # I'm adding this check to provide a clear error message\n",
        "    # if the dataset path is incorrect, which is a crucial\n",
        "    # check before proceeding with data loading.\n",
        "    # The subtask can still be finished as failure if needed.\n",
        "\n",
        "\n",
        "# Define image dimensions\n",
        "IMG_HEIGHT = 224\n",
        "IMG_WIDTH = 224\n",
        "IMG_SIZE = (IMG_HEIGHT, IMG_WIDTH)\n",
        "\n",
        "# Define training constants\n",
        "BATCH_SIZE = 32  # Reduced batch size for potential OOM\n",
        "HEAD_ONLY_LR = 0.001\n",
        "FINE_TUNE_LR = 0.0001\n",
        "HEAD_ONLY_EPOCHS = 10\n",
        "FINE_TUNE_EPOCHS = 20\n",
        "\n",
        "# Define artifact paths\n",
        "ARTIFACTS_DIR = \"./artifacts\"\n",
        "CHECKPOINT_DIR = os.path.join(ARTIFACTS_DIR, \"checkpoints\")\n",
        "BEST_MODEL_PATH = os.path.join(ARTIFACTS_DIR, \"best_model.pth\")\n",
        "ONNX_MODEL_PATH = os.path.join(ARTIFACTS_DIR, \"model.onnx\")\n",
        "INFERENCE_SCRIPT_PATH = os.path.join(ARTIFACTS_DIR, \"inference_script.py\")\n",
        "README_PATH = os.path.join(ARTIFACTS_DIR, \"README.md\")\n",
        "\n",
        "# Create artifact directories if they don't exist\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"Constants defined and artifact directories created.\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Constants defined and artifact directories created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55a7955b"
      },
      "source": [
        "## Data verification and preparation\n",
        "\n",
        "### Subtask:\n",
        "Verify dataset path, list classes, count images per class, assert minimum classes and images per class. Create deterministic train/val/test splits and save them as CSV files.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "badc6fcd"
      },
      "source": [
        "**Reasoning**:\n",
        "Verify the dataset path, list classes, count images per class, assert minimum classes and images per class, create a list of all image file paths and their corresponding class labels, deterministically split the data into train/val/test sets, and save the splits as CSV files. This covers steps 1 through 8 of the instructions and can be accomplished in a single code block.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "4a719de1",
        "outputId": "884d9e1b-01a8-480b-a067-b6fb22d15754"
      },
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. Verify that the dataset path DATA_DIR exists.\n",
        "if not os.path.exists(DATA_DIR):\n",
        "    print(f\"Error: Dataset path not found at {DATA_DIR}.\")\n",
        "    # Indicate failure by not proceeding further\n",
        "else:\n",
        "    print(f\"Dataset path verified: {DATA_DIR}\")\n",
        "\n",
        "    # 2. List all the subdirectories in DATA_DIR. These subdirectories represent the different classes.\n",
        "    classes = [d for d in os.listdir(DATA_DIR) if os.path.isdir(os.path.join(DATA_DIR, d))]\n",
        "    class_names = sorted(classes)\n",
        "    print(f\"Detected classes: {class_names}\")\n",
        "\n",
        "    # 3. Count the number of images in each class subdirectory.\n",
        "    image_counts = {}\n",
        "    for class_name in class_names:\n",
        "        class_dir = os.path.join(DATA_DIR, class_name)\n",
        "        # Assuming image files have common extensions like .jpg, .jpeg, .png, etc.\n",
        "        # Adjust the pattern if necessary\n",
        "        image_paths = glob.glob(os.path.join(class_dir, \"*.jpg\")) + \\\n",
        "                      glob.glob(os.path.join(class_dir, \"*.jpeg\")) + \\\n",
        "                      glob.glob(os.path.join(class_dir, \"*.png\"))\n",
        "        image_counts[class_name] = len(image_paths)\n",
        "        print(f\"Class '{class_name}': {image_counts[class_name]} images\")\n",
        "\n",
        "    # 4. Assert that the number of classes is greater than or equal to 2.\n",
        "    min_classes = 2\n",
        "    if len(class_names) < min_classes:\n",
        "        print(f\"Error: Number of classes ({len(class_names)}) is less than the minimum required ({min_classes}).\")\n",
        "    else:\n",
        "        print(f\"Number of classes assertion passed ({len(class_names)} >= {min_classes}).\")\n",
        "\n",
        "        # 5. Assert that each class has at least a minimum number of images.\n",
        "        min_images_per_class = 10  # Example minimum\n",
        "        classes_with_insufficient_images = [class_name for class_name, count in image_counts.items() if count < min_images_per_class]\n",
        "\n",
        "        if classes_with_insufficient_images:\n",
        "            print(f\"Error: The following classes have fewer than {min_images_per_class} images: {classes_with_insufficient_images}\")\n",
        "        else:\n",
        "            print(f\"Minimum images per class assertion passed (each class has at least {min_images_per_class} images).\")\n",
        "\n",
        "            # 6. Create a list of all image file paths along with their corresponding class labels.\n",
        "            all_image_paths = []\n",
        "            all_image_labels = []\n",
        "            for class_name in class_names:\n",
        "                class_dir = os.path.join(DATA_DIR, class_name)\n",
        "                image_paths = glob.glob(os.path.join(class_dir, \"*.jpg\")) + \\\n",
        "                              glob.glob(os.path.join(class_dir, \"*.jpeg\")) + \\\n",
        "                              glob.glob(os.path.join(class_dir, \"*.png\"))\n",
        "                all_image_paths.extend(image_paths)\n",
        "                all_image_labels.extend([class_name] * len(image_paths))\n",
        "\n",
        "            print(f\"Total images found: {len(all_image_paths)}\")\n",
        "\n",
        "            # 7. Deterministically split the list of image paths and labels.\n",
        "            # Use a fixed random state for reproducibility\n",
        "            random_state = 42\n",
        "            train_size = 0.7\n",
        "            val_size = 0.15\n",
        "            test_size = 0.15 # Adjusted to make sum 1.0\n",
        "\n",
        "            # Split into train and temp (for val/test)\n",
        "            train_paths, temp_paths, train_labels, temp_labels = train_test_split(\n",
        "                all_image_paths, all_image_labels, train_size=train_size, stratify=all_image_labels, random_state=random_state\n",
        "            )\n",
        "\n",
        "            # Split temp into validation and test\n",
        "            # Adjust test_size proportion based on the remaining temp data\n",
        "            test_size_proportion = test_size / (val_size + test_size) # 0.15 / 0.30 = 0.5\n",
        "            val_paths, test_paths, val_labels, test_labels = train_test_split(\n",
        "                temp_paths, temp_labels, test_size=test_size_proportion, stratify=temp_labels, random_state=random_state\n",
        "            )\n",
        "\n",
        "            print(f\"Train set size: {len(train_paths)}\")\n",
        "            print(f\"Validation set size: {len(val_paths)}\")\n",
        "            print(f\"Test set size: {len(test_paths)}\")\n",
        "\n",
        "            # 8. Save the train, validation, and test splits as separate CSV files.\n",
        "            train_df = pd.DataFrame({'image_path': train_paths, 'label': train_labels})\n",
        "            val_df = pd.DataFrame({'image_path': val_paths, 'label': val_labels})\n",
        "            test_df = pd.DataFrame({'image_path': test_paths, 'label': test_labels})\n",
        "\n",
        "            train_df.to_csv(os.path.join(ARTIFACTS_DIR, 'train_split.csv'), index=False)\n",
        "            val_df.to_csv(os.path.join(ARTIFACTS_DIR, 'val_split.csv'), index=False)\n",
        "            test_df.to_csv(os.path.join(ARTIFACTS_DIR, 'test_split.csv'), index=False)\n",
        "\n",
        "            print(\"Train, validation, and test splits saved as CSV files in the artifacts directory.\")\n",
        "\n",
        "            # Display the first few rows of the train split dataframe\n",
        "            display(train_df.head())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pandas'",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mglob\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# 1. Verify that the dataset path DATA_DIR exists.\u001b[39;00m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "622ee6fc"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because the pandas library was not imported. Add the import statement for pandas.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "178ln5RrLnw1",
        "outputId": "0f99d946-007c-48e4-a9a5-df5bc4b8705d"
      },
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. Verify that the dataset path DATA_DIR exists.\n",
        "if not os.path.exists(DATA_DIR):\n",
        "    print(f\"Error: Dataset path not found at {DATA_DIR}.\")\n",
        "    # Indicate failure by not proceeding further\n",
        "else:\n",
        "    print(f\"Dataset path verified: {DATA_DIR}\")\n",
        "\n",
        "    # 2. List all the subdirectories in DATA_DIR. These subdirectories represent the different classes.\n",
        "    classes = [d for d in os.listdir(DATA_DIR) if os.path.isdir(os.path.join(DATA_DIR, d))]\n",
        "    class_names = sorted(classes)\n",
        "    print(f\"Detected classes: {class_names}\")\n",
        "\n",
        "    # 3. Count the number of images in each class subdirectory.\n",
        "    image_counts = {}\n",
        "    for class_name in class_names:\n",
        "        class_dir = os.path.join(DATA_DIR, class_name)\n",
        "        # Assuming image files have common extensions like .jpg, .jpeg, .png, etc.\n",
        "        # Adjust the pattern if necessary\n",
        "        image_paths = glob.glob(os.path.join(class_dir, \"*.jpg\")) + \\\n",
        "                      glob.glob(os.path.join(class_dir, \"*.jpeg\")) + \\\n",
        "                      glob.glob(os.path.join(class_dir, \"*.png\"))\n",
        "        image_counts[class_name] = len(image_paths)\n",
        "        print(f\"Class '{class_name}': {image_counts[class_name]} images\")\n",
        "\n",
        "    # 4. Assert that the number of classes is greater than or equal to 2.\n",
        "    min_classes = 2\n",
        "    if len(class_names) < min_classes:\n",
        "        print(f\"Error: Number of classes ({len(class_names)}) is less than the minimum required ({min_classes}).\")\n",
        "    else:\n",
        "        print(f\"Number of classes assertion passed ({len(class_names)} >= {min_classes}).\")\n",
        "\n",
        "        # 5. Assert that each class has at least a minimum number of images.\n",
        "        min_images_per_class = 10  # Example minimum\n",
        "        classes_with_insufficient_images = [class_name for class_name, count in image_counts.items() if count < min_images_per_class]\n",
        "\n",
        "        if classes_with_insufficient_images:\n",
        "            print(f\"Error: The following classes have fewer than {min_images_per_class} images: {classes_with_insufficient_images}\")\n",
        "        else:\n",
        "            print(f\"Minimum images per class assertion passed (each class has at least {min_images_per_class} images).\")\n",
        "\n",
        "            # 6. Create a list of all image file paths along with their corresponding class labels.\n",
        "            all_image_paths = []\n",
        "            all_image_labels = []\n",
        "            for class_name in class_names:\n",
        "                class_dir = os.path.join(DATA_DIR, class_name)\n",
        "                image_paths = glob.glob(os.path.join(class_dir, \"*.jpg\")) + \\\n",
        "                              glob.glob(os.path.join(class_dir, \"*.jpeg\")) + \\\n",
        "                              glob.glob(os.path.join(class_dir, \"*.png\"))\n",
        "                all_image_paths.extend(image_paths)\n",
        "                all_image_labels.extend([class_name] * len(image_paths))\n",
        "\n",
        "            print(f\"Total images found: {len(all_image_paths)}\")\n",
        "\n",
        "            # 7. Deterministically split the list of image paths and labels.\n",
        "            # Use a fixed random state for reproducibility\n",
        "            random_state = 42\n",
        "            train_size = 0.7\n",
        "            val_size = 0.15\n",
        "            test_size = 0.15 # Adjusted to make sum 1.0\n",
        "\n",
        "            # Split into train and temp (for val/test)\n",
        "            train_paths, temp_paths, train_labels, temp_labels = train_test_split(\n",
        "                all_image_paths, all_image_labels, train_size=train_size, stratify=all_image_labels, random_state=random_state\n",
        "            )\n",
        "\n",
        "            # Split temp into validation and test\n",
        "            # Adjust test_size proportion based on the remaining temp data\n",
        "            test_size_proportion = test_size / (val_size + test_size) # 0.15 / 0.30 = 0.5\n",
        "            val_paths, test_paths, val_labels, test_labels = train_test_split(\n",
        "                temp_paths, temp_labels, test_size=test_size_proportion, stratify=temp_labels, random_state=random_state\n",
        "            )\n",
        "\n",
        "            print(f\"Train set size: {len(train_paths)}\")\n",
        "            print(f\"Validation set size: {len(val_paths)}\")\n",
        "            print(f\"Test set size: {len(test_paths)}\")\n",
        "\n",
        "            # 8. Save the train, validation, and test splits as separate CSV files.\n",
        "            train_df = pd.DataFrame({'image_path': train_paths, 'label': train_labels})\n",
        "            val_df = pd.DataFrame({'image_path': val_paths, 'label': val_labels})\n",
        "            test_df = pd.DataFrame({'image_path': test_paths, 'label': test_labels})\n",
        "\n",
        "            train_df.to_csv(os.path.join(ARTIFACTS_DIR, 'train_split.csv'), index=False)\n",
        "            val_df.to_csv(os.path.join(ARTIFACTS_DIR, 'val_split.csv'), index=False)\n",
        "            test_df.to_csv(os.path.join(ARTIFACTS_DIR, 'test_split.csv'), index=False)\n",
        "\n",
        "            print(\"Train, validation, and test splits saved as CSV files in the artifacts directory.\")\n",
        "\n",
        "            # Display the first few rows of the train split dataframe\n",
        "            display(train_df.head())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pandas'",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mglob\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "957ad863"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because the pandas library was not imported. Add the import statement for pandas. Also, add import for os which is used in the code.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "kKknH2phLrDT",
        "outputId": "536eae64-2650-47c0-e34b-0f75b323e4a8"
      },
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. Verify that the dataset path DATA_DIR exists.\n",
        "if not os.path.exists(DATA_DIR):\n",
        "    print(f\"Error: Dataset path not found at {DATA_DIR}.\")\n",
        "    # Indicate failure by not proceeding further\n",
        "else:\n",
        "    print(f\"Dataset path verified: {DATA_DIR}\")\n",
        "\n",
        "    # 2. List all the subdirectories in DATA_DIR. These subdirectories represent the different classes.\n",
        "    classes = [d for d in os.listdir(DATA_DIR) if os.path.isdir(os.path.join(DATA_DIR, d))]\n",
        "    class_names = sorted(classes)\n",
        "    print(f\"Detected classes: {class_names}\")\n",
        "\n",
        "    # 3. Count the number of images in each class subdirectory.\n",
        "    image_counts = {}\n",
        "    for class_name in class_names:\n",
        "        class_dir = os.path.join(DATA_DIR, class_name)\n",
        "        # Assuming image files have common extensions like .jpg, .jpeg, .png, etc.\n",
        "        # Adjust the pattern if necessary\n",
        "        image_paths = glob.glob(os.path.join(class_dir, \"*.jpg\")) + \\\n",
        "                      glob.glob(os.path.join(class_dir, \"*.jpeg\")) + \\\n",
        "                      glob.glob(os.path.join(class_dir, \"*.png\"))\n",
        "        image_counts[class_name] = len(image_paths)\n",
        "        print(f\"Class '{class_name}': {image_counts[class_name]} images\")\n",
        "\n",
        "    # 4. Assert that the number of classes is greater than or equal to 2.\n",
        "    min_classes = 2\n",
        "    if len(class_names) < min_classes:\n",
        "        print(f\"Error: Number of classes ({len(class_names)}) is less than the minimum required ({min_classes}).\")\n",
        "    else:\n",
        "        print(f\"Number of classes assertion passed ({len(class_names)} >= {min_classes}).\")\n",
        "\n",
        "        # 5. Assert that each class has at least a minimum number of images.\n",
        "        min_images_per_class = 10  # Example minimum\n",
        "        classes_with_insufficient_images = [class_name for class_name, count in image_counts.items() if count < min_images_per_class]\n",
        "\n",
        "        if classes_with_insufficient_images:\n",
        "            print(f\"Error: The following classes have fewer than {min_images_per_class} images: {classes_with_insufficient_images}\")\n",
        "        else:\n",
        "            print(f\"Minimum images per class assertion passed (each class has at least {min_images_per_class} images).\")\n",
        "\n",
        "            # 6. Create a list of all image file paths along with their corresponding class labels.\n",
        "            all_image_paths = []\n",
        "            all_image_labels = []\n",
        "            for class_name in class_names:\n",
        "                class_dir = os.path.join(DATA_DIR, class_name)\n",
        "                image_paths = glob.glob(os.path.join(class_dir, \"*.jpg\")) + \\\n",
        "                              glob.glob(os.path.join(class_dir, \"*.jpeg\")) + \\\n",
        "                              glob.glob(os.path.join(class_dir, \"*.png\"))\n",
        "                all_image_paths.extend(image_paths)\n",
        "                all_image_labels.extend([class_name] * len(image_paths))\n",
        "\n",
        "            print(f\"Total images found: {len(all_image_paths)}\")\n",
        "\n",
        "            # 7. Deterministically split the list of image paths and labels.\n",
        "            # Use a fixed random state for reproducibility\n",
        "            random_state = 42\n",
        "            train_size = 0.7\n",
        "            val_size = 0.15\n",
        "            test_size = 0.15 # Adjusted to make sum 1.0\n",
        "\n",
        "            # Split into train and temp (for val/test)\n",
        "            train_paths, temp_paths, train_labels, temp_labels = train_test_split(\n",
        "                all_image_paths, all_image_labels, train_size=train_size, stratify=all_image_labels, random_state=random_state\n",
        "            )\n",
        "\n",
        "            # Split temp into validation and test\n",
        "            # Adjust test_size proportion based on the remaining temp data\n",
        "            test_size_proportion = test_size / (val_size + test_size) # 0.15 / 0.30 = 0.5\n",
        "            val_paths, test_paths, val_labels, test_labels = train_test_split(\n",
        "                temp_paths, temp_labels, test_size=test_size_proportion, stratify=temp_labels, random_state=random_state\n",
        "            )\n",
        "\n",
        "            print(f\"Train set size: {len(train_paths)}\")\n",
        "            print(f\"Validation set size: {len(val_paths)}\")\n",
        "            print(f\"Test set size: {len(test_paths)}\")\n",
        "\n",
        "            # 8. Save the train, validation, and test splits as separate CSV files.\n",
        "            train_df = pd.DataFrame({'image_path': train_paths, 'label': train_labels})\n",
        "            val_df = pd.DataFrame({'image_path': val_paths, 'label': val_labels})\n",
        "            test_df = pd.DataFrame({'image_path': test_paths, 'label': test_labels})\n",
        "\n",
        "            train_df.to_csv(os.path.join(ARTIFACTS_DIR, 'train_split.csv'), index=False)\n",
        "            val_df.to_csv(os.path.join(ARTIFACTS_DIR, 'val_split.csv'), index=False)\n",
        "            test_df.to_csv(os.path.join(ARTIFACTS_DIR, 'test_split.csv'), index=False)\n",
        "\n",
        "            print(\"Train, validation, and test splits saved as CSV files in the artifacts directory.\")\n",
        "\n",
        "            # Display the first few rows of the train split dataframe\n",
        "            display(train_df.head())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pandas'",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mglob\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "051c5f0f"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because the pandas library was not imported. Although the import statement was present, it seems the environment is not recognizing it. I will try importing pandas again in case of an environment issue and include all necessary imports in one block to ensure they are present.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "LyvR8tqELvMx",
        "outputId": "386fdadb-b247-4c6a-8952-f9cfa08a27f8"
      },
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. Verify that the dataset path DATA_DIR exists.\n",
        "if not os.path.exists(DATA_DIR):\n",
        "    print(f\"Error: Dataset path not found at {DATA_DIR}.\")\n",
        "    # Indicate failure by not proceeding further\n",
        "else:\n",
        "    print(f\"Dataset path verified: {DATA_DIR}\")\n",
        "\n",
        "    # 2. List all the subdirectories in DATA_DIR. These subdirectories represent the different classes.\n",
        "    classes = [d for d in os.listdir(DATA_DIR) if os.path.isdir(os.path.join(DATA_DIR, d))]\n",
        "    class_names = sorted(classes)\n",
        "    print(f\"Detected classes: {class_names}\")\n",
        "\n",
        "    # 3. Count the number of images in each class subdirectory.\n",
        "    image_counts = {}\n",
        "    for class_name in class_names:\n",
        "        class_dir = os.path.join(DATA_DIR, class_name)\n",
        "        # Assuming image files have common extensions like .jpg, .jpeg, .png, etc.\n",
        "        # Adjust the pattern if necessary\n",
        "        image_paths = glob.glob(os.path.join(class_dir, \"*.jpg\")) + \\\n",
        "                      glob.glob(os.path.join(class_dir, \"*.jpeg\")) + \\\n",
        "                      glob.glob(os.path.join(class_dir, \"*.png\"))\n",
        "        image_counts[class_name] = len(image_paths)\n",
        "        print(f\"Class '{class_name}': {image_counts[class_name]} images\")\n",
        "\n",
        "    # 4. Assert that the number of classes is greater than or equal to 2.\n",
        "    min_classes = 2\n",
        "    if len(class_names) < min_classes:\n",
        "        print(f\"Error: Number of classes ({len(class_names)}) is less than the minimum required ({min_classes}).\")\n",
        "    else:\n",
        "        print(f\"Number of classes assertion passed ({len(class_names)} >= {min_classes}).\")\n",
        "\n",
        "        # 5. Assert that each class has at least a minimum number of images.\n",
        "        min_images_per_class = 10  # Example minimum\n",
        "        classes_with_insufficient_images = [class_name for class_name, count in image_counts.items() if count < min_images_per_class]\n",
        "\n",
        "        if classes_with_insufficient_images:\n",
        "            print(f\"Error: The following classes have fewer than {min_images_per_class} images: {classes_with_insufficient_images}\")\n",
        "        else:\n",
        "            print(f\"Minimum images per class assertion passed (each class has at least {min_images_per_class} images).\")\n",
        "\n",
        "            # 6. Create a list of all image file paths along with their corresponding class labels.\n",
        "            all_image_paths = []\n",
        "            all_image_labels = []\n",
        "            for class_name in class_names:\n",
        "                class_dir = os.path.join(DATA_DIR, class_name)\n",
        "                image_paths = glob.glob(os.path.join(class_dir, \"*.jpg\")) + \\\n",
        "                              glob.glob(os.path.join(class_dir, \"*.jpeg\")) + \\\n",
        "                              glob.glob(os.path.join(class_dir, \"*.png\"))\n",
        "                all_image_paths.extend(image_paths)\n",
        "                all_image_labels.extend([class_name] * len(image_paths))\n",
        "\n",
        "            print(f\"Total images found: {len(all_image_paths)}\")\n",
        "\n",
        "            # 7. Deterministically split the list of image paths and labels.\n",
        "            # Use a fixed random state for reproducibility\n",
        "            random_state = 42\n",
        "            train_size = 0.7\n",
        "            val_size = 0.15\n",
        "            test_size = 0.15 # Adjusted to make sum 1.0\n",
        "\n",
        "            # Split into train and temp (for val/test)\n",
        "            train_paths, temp_paths, train_labels, temp_labels = train_test_split(\n",
        "                all_image_paths, all_image_labels, train_size=train_size, stratify=all_image_labels, random_state=random_state\n",
        "            )\n",
        "\n",
        "            # Split temp into validation and test\n",
        "            # Adjust test_size proportion based on the remaining temp data\n",
        "            test_size_proportion = test_size / (val_size + test_size) # 0.15 / 0.30 = 0.5\n",
        "            val_paths, test_paths, val_labels, test_labels = train_test_split(\n",
        "                temp_paths, temp_labels, test_size=test_size_proportion, stratify=temp_labels, random_state=random_state\n",
        "            )\n",
        "\n",
        "            print(f\"Train set size: {len(train_paths)}\")\n",
        "            print(f\"Validation set size: {len(val_paths)}\")\n",
        "            print(f\"Test set size: {len(test_paths)}\")\n",
        "\n",
        "            # 8. Save the train, validation, and test splits as separate CSV files.\n",
        "            train_df = pd.DataFrame({'image_path': train_paths, 'label': train_labels})\n",
        "            val_df = pd.DataFrame({'image_path': val_paths, 'label': val_labels})\n",
        "            test_df = pd.DataFrame({'image_path': test_paths, 'label': test_labels})\n",
        "\n",
        "            train_df.to_csv(os.path.join(ARTIFACTS_DIR, 'train_split.csv'), index=False)\n",
        "            val_df.to_csv(os.path.join(ARTIFACTS_DIR, 'val_split.csv'), index=False)\n",
        "            test_df.to_csv(os.path.join(ARTIFACTS_DIR, 'test_split.csv'), index=False)\n",
        "\n",
        "            print(\"Train, validation, and test splits saved as CSV files in the artifacts directory.\")\n",
        "\n",
        "            # Display the first few rows of the train split dataframe\n",
        "            display(train_df.head())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pandas'",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mglob\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49d2c747"
      },
      "source": [
        "## Data verification and preparation retry 1\n",
        "\n",
        "### Subtask:\n",
        "Verify dataset path, list classes, count images per class, assert minimum classes and images per class. Create deterministic train/val/test splits and save them as CSV files.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0530eab7"
      },
      "source": [
        "**Reasoning**:\n",
        "Since the previous attempts failed due to a `ModuleNotFoundError: No module named 'pandas'`, and based on the history, the installation of pandas was attempted but failed. I will try installing pandas again using pip within a separate cell to ensure it's available before proceeding with the data verification and splitting steps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac229804",
        "outputId": "30b4fa85-91d2-4feb-f644-31e6da2b1947"
      },
      "source": [
        "%pip install pandas"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pandas\n",
            "  Using cached pandas-2.3.3-cp310-cp310-win_amd64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: numpy>=1.22.4 in c:\\programdata\\miniconda3\\envs\\hack1\\lib\\site-packages (from pandas) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\miniconda3\\envs\\hack1\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
            "Collecting pytz>=2020.1 (from pandas)\n",
            "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas)\n",
            "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: six>=1.5 in c:\\programdata\\miniconda3\\envs\\hack1\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Using cached pandas-2.3.3-cp310-cp310-win_amd64.whl (11.3 MB)\n",
            "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
            "Installing collected packages: pytz, tzdata, pandas\n",
            "\n",
            "   ---------------------------------------- 0/3 [pytz]\n",
            "   ------------- -------------------------- 1/3 [tzdata]\n",
            "   ------------- -------------------------- 1/3 [tzdata]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   ---------------------------------------- 3/3 [pandas]\n",
            "\n",
            "Successfully installed pandas-2.3.3 pytz-2025.2 tzdata-2025.2\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c59ec19"
      },
      "source": [
        "**Reasoning**:\n",
        "Since pandas was successfully installed in the previous step, I will now proceed with the data verification and splitting steps as outlined in the subtask instructions. This involves verifying the dataset path, listing classes, counting images, performing assertions on the number of classes and images per class, creating the train/validation/test splits deterministically, creating DataFrames for the splits, and saving them as CSV files. I will also print the split sizes and display the head of the training DataFrame.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        },
        "id": "f7092f6b",
        "outputId": "1878fa3e-85f1-4263-e16b-8cef40c27fa9"
      },
      "source": [
        "import glob\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd # Explicitly import pandas again after installation\n",
        "\n",
        "# 1. Verify that the dataset path DATA_DIR exists.\n",
        "if not os.path.exists(DATA_DIR):\n",
        "    print(f\"Error: Dataset path not found at {DATA_DIR}.\")\n",
        "    # Indicate failure by not proceeding further\n",
        "else:\n",
        "    print(f\"Dataset path verified: {DATA_DIR}\")\n",
        "\n",
        "    # 2. List all the subdirectories in DATA_DIR. These subdirectories represent the different classes.\n",
        "    classes = [d for d in os.listdir(DATA_DIR) if os.path.isdir(os.path.join(DATA_DIR, d))]\n",
        "    class_names = sorted(classes)\n",
        "    print(f\"Detected classes: {class_names}\")\n",
        "\n",
        "    # 3. Count the number of images in each class subdirectory.\n",
        "    image_counts = {}\n",
        "    for class_name in class_names:\n",
        "        class_dir = os.path.join(DATA_DIR, class_name)\n",
        "        # Assuming image files have common extensions like .jpg, .jpeg, .png, etc.\n",
        "        # Adjust the pattern if necessary\n",
        "        image_paths = glob.glob(os.path.join(class_dir, \"*.jpg\")) + \\\n",
        "                      glob.glob(os.path.join(class_dir, \"*.jpeg\")) + \\\n",
        "                      glob.glob(os.path.join(class_dir, \"*.png\"))\n",
        "        image_counts[class_name] = len(image_paths)\n",
        "        print(f\"Class '{class_name}': {image_counts[class_name]} images\")\n",
        "\n",
        "    # 4. Assert that the number of classes is greater than or equal to 2.\n",
        "    min_classes = 2\n",
        "    if len(class_names) < min_classes:\n",
        "        print(f\"Error: Number of classes ({len(class_names)}) is less than the minimum required ({min_classes}).\")\n",
        "    else:\n",
        "        print(f\"Number of classes assertion passed ({len(class_names)} >= {min_classes}).\")\n",
        "\n",
        "        # 5. Assert that each class has at least a minimum number of images.\n",
        "        min_images_per_class = 10  # Example minimum\n",
        "        classes_with_insufficient_images = [class_name for class_name, count in image_counts.items() if count < min_images_per_class]\n",
        "\n",
        "        if classes_with_insufficient_images:\n",
        "            print(f\"Error: The following classes have fewer than {min_images_per_class} images: {classes_with_insufficient_images}\")\n",
        "        else:\n",
        "            print(f\"Minimum images per class assertion passed (each class has at least {min_images_per_class} images).\")\n",
        "\n",
        "            # 6. Create a list of all image file paths along with their corresponding class labels.\n",
        "            all_image_paths = []\n",
        "            all_image_labels = []\n",
        "            for class_name in class_names:\n",
        "                class_dir = os.path.join(DATA_DIR, class_name)\n",
        "                image_paths = glob.glob(os.path.join(class_dir, \"*.jpg\")) + \\\n",
        "                              glob.glob(os.path.join(class_dir, \"*.jpeg\")) + \\\n",
        "                              glob.glob(os.path.join(class_dir, \"*.png\"))\n",
        "                all_image_paths.extend(image_paths)\n",
        "                all_image_labels.extend([class_name] * len(image_paths))\n",
        "\n",
        "            print(f\"Total images found: {len(all_image_paths)}\")\n",
        "\n",
        "            # 7. Deterministically split the list of image paths and labels.\n",
        "            # Use a fixed random state for reproducibility\n",
        "            random_state = 42\n",
        "            train_size = 0.7\n",
        "            val_size = 0.15\n",
        "            test_size = 0.15 # Adjusted to make sum 1.0\n",
        "\n",
        "            # Split into train and temp (for val/test)\n",
        "            train_paths, temp_paths, train_labels, temp_labels = train_test_split(\n",
        "                all_image_paths, all_image_labels, train_size=train_size, stratify=all_image_labels, random_state=random_state\n",
        "            )\n",
        "\n",
        "            # Split temp into validation and test\n",
        "            # Adjust test_size proportion based on the remaining temp data\n",
        "            test_size_proportion = test_size / (val_size + test_size) # 0.15 / 0.30 = 0.5\n",
        "            val_paths, test_paths, val_labels, test_labels = train_test_split(\n",
        "                temp_paths, temp_labels, test_size=test_size_proportion, stratify=temp_labels, random_state=random_state\n",
        "            )\n",
        "\n",
        "            print(f\"Train set size: {len(train_paths)}\")\n",
        "            print(f\"Validation set size: {len(val_paths)}\")\n",
        "            print(f\"Test set size: {len(test_paths)}\")\n",
        "\n",
        "            # 8. Save the train, validation, and test splits as separate CSV files.\n",
        "            train_df = pd.DataFrame({'image_path': train_paths, 'label': train_labels})\n",
        "            val_df = pd.DataFrame({'image_path': val_paths, 'label': val_labels})\n",
        "            test_df = pd.DataFrame({'image_path': test_paths, 'label': test_labels})\n",
        "\n",
        "            train_df.to_csv(os.path.join(ARTIFACTS_DIR, 'train_split.csv'), index=False)\n",
        "            val_df.to_csv(os.path.join(ARTIFACTS_DIR, 'val_split.csv'), index=False)\n",
        "            test_df.to_csv(os.path.join(ARTIFACTS_DIR, 'test_split.csv'), index=False)\n",
        "\n",
        "            print(\"Train, validation, and test splits saved as CSV files in the artifacts directory.\")\n",
        "\n",
        "            # Display the first few rows of the train split dataframe\n",
        "            display(train_df.head())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset path verified: C:\\Windows\\System32\\curated_indian_food_dataset\n",
            "Detected classes: ['biryani', 'burger', 'chai', 'chapati', 'cholebhature', 'dahl', 'dhokla', 'dosa', 'friedrice', 'gulabjamun', 'idli', 'jalebi', 'kaathirolls', 'kadaipaneer', 'kulfi', 'momos', 'naan', 'paanipuri', 'pakode', 'pavbhaji', 'pizza', 'poha', 'rolls', 'samosa', 'vadapav']\n",
            "Class 'biryani': 150 images\n",
            "Class 'burger': 150 images\n",
            "Class 'chai': 150 images\n",
            "Class 'chapati': 150 images\n",
            "Class 'cholebhature': 150 images\n",
            "Class 'dahl': 150 images\n",
            "Class 'dhokla': 150 images\n",
            "Class 'dosa': 150 images\n",
            "Class 'friedrice': 150 images\n",
            "Class 'gulabjamun': 150 images\n",
            "Class 'idli': 150 images\n",
            "Class 'jalebi': 150 images\n",
            "Class 'kaathirolls': 150 images\n",
            "Class 'kadaipaneer': 150 images\n",
            "Class 'kulfi': 133 images\n",
            "Class 'momos': 150 images\n",
            "Class 'naan': 150 images\n",
            "Class 'paanipuri': 150 images\n",
            "Class 'pakode': 150 images\n",
            "Class 'pavbhaji': 150 images\n",
            "Class 'pizza': 150 images\n",
            "Class 'poha': 150 images\n",
            "Class 'rolls': 150 images\n",
            "Class 'samosa': 150 images\n",
            "Class 'vadapav': 150 images\n",
            "Number of classes assertion passed (25 >= 2).\n",
            "Minimum images per class assertion passed (each class has at least 10 images).\n",
            "Total images found: 3733\n",
            "Train set size: 2613\n",
            "Validation set size: 560\n",
            "Test set size: 560\n",
            "Train, validation, and test splits saved as CSV files in the artifacts directory.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                          image_path        label\n",
              "0  C:\\Windows\\System32\\curated_indian_food_datase...      chapati\n",
              "1  C:\\Windows\\System32\\curated_indian_food_datase...  kaathirolls\n",
              "2  C:\\Windows\\System32\\curated_indian_food_datase...        kulfi\n",
              "3  C:\\Windows\\System32\\curated_indian_food_datase...         naan\n",
              "4  C:\\Windows\\System32\\curated_indian_food_datase...      vadapav"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>C:\\Windows\\System32\\curated_indian_food_datase...</td>\n",
              "      <td>chapati</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>C:\\Windows\\System32\\curated_indian_food_datase...</td>\n",
              "      <td>kaathirolls</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>C:\\Windows\\System32\\curated_indian_food_datase...</td>\n",
              "      <td>kulfi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>C:\\Windows\\System32\\curated_indian_food_datase...</td>\n",
              "      <td>naan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>C:\\Windows\\System32\\curated_indian_food_datase...</td>\n",
              "      <td>vadapav</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2d31520"
      },
      "source": [
        "## Dataloaders\n",
        "\n",
        "### Subtask:\n",
        "Create PyTorch DataLoaders for train, validation, and test sets with appropriate transformations (resize, normalize, augmentation). Verify shapes, dtype, min/max, and label range of the first batch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1655d1ca"
      },
      "source": [
        "**Reasoning**:\n",
        "Define image transformations, create a custom PyTorch Dataset class to handle image loading and transformations while skipping corrupt images, and create DataLoaders for train, validation, and test sets. Then, get the first batch and verify its shape, dtype, min/max values, and label range.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2a418b8d",
        "outputId": "0e39f8f9-4f4b-424b-c91a-d97b856de372"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# 1. Define the image transformations\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(IMG_SIZE),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_test_transforms = transforms.Compose([\n",
        "    transforms.Resize(IMG_SIZE), # Use Resize for validation/test\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Get the list of unique class names from the training data\n",
        "class_names = sorted(train_df['label'].unique())\n",
        "class_to_idx = {cls_name: i for i, cls_name in enumerate(class_names)}\n",
        "num_classes = len(class_names)\n",
        "print(f\"Number of classes: {num_classes}\")\n",
        "print(f\"Class names and indices: {class_to_idx}\")\n",
        "\n",
        "\n",
        "# 2. Create a custom PyTorch Dataset class\n",
        "class FoodDataset(Dataset):\n",
        "    def __init__(self, dataframe, transform=None, class_to_idx=None):\n",
        "        self.dataframe = dataframe\n",
        "        self.transform = transform\n",
        "        self.class_to_idx = class_to_idx\n",
        "        self.image_paths = dataframe['image_path'].tolist()\n",
        "        self.labels = dataframe['label'].tolist()\n",
        "        print(f\"Initialized dataset with {len(self.image_paths)} images.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        label_name = self.labels[idx]\n",
        "\n",
        "        try:\n",
        "            img = Image.open(img_path).convert('RGB') # Convert to RGB to handle grayscale images\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "\n",
        "            # Convert label name to index\n",
        "            label = self.class_to_idx[label_name]\n",
        "\n",
        "            return img, label\n",
        "        except Exception as e:\n",
        "            # Handle corrupt images or loading errors\n",
        "            print(f\"Skipping corrupt or unreadable image: {img_path} - {e}\")\n",
        "            # Return None to indicate a skipped image. This will be handled in the DataLoader's collate_fn.\n",
        "            return None, None\n",
        "\n",
        "# Custom collate function to filter out None values from skipped images\n",
        "def collate_fn(batch):\n",
        "    batch = list(filter(lambda x: x[0] is not None, batch))\n",
        "    if not batch:\n",
        "        return None, None # Return None if the entire batch is empty\n",
        "    return torch.utils.data.dataloader.default_collate(batch)\n",
        "\n",
        "\n",
        "# Create Dataset instances\n",
        "train_dataset = FoodDataset(train_df, transform=train_transforms, class_to_idx=class_to_idx)\n",
        "val_dataset = FoodDataset(val_df, transform=val_test_transforms, class_to_idx=class_to_idx)\n",
        "test_dataset = FoodDataset(test_df, transform=val_test_transforms, class_to_idx=class_to_idx)\n",
        "\n",
        "# 3. Create PyTorch DataLoaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, collate_fn=collate_fn) # num_workers=0 for Windows compatibility\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
        "\n",
        "print(\"DataLoaders created.\")\n",
        "\n",
        "# 4. Get the first batch of data from the training DataLoader\n",
        "images, labels = next(iter(train_dataloader))\n",
        "\n",
        "# 5. Verify shape, dtype, min/max values of the image tensor\n",
        "print(\"\\nVerifying first batch:\")\n",
        "print(f\"Image batch shape: {images.shape}\")\n",
        "print(f\"Image batch dtype: {images.dtype}\")\n",
        "print(f\"Image batch min value: {torch.min(images)}\")\n",
        "print(f\"Image batch max value: {torch.max(images)}\")\n",
        "\n",
        "# Assert expected image properties\n",
        "assert images.shape == (labels.shape[0], 3, IMG_HEIGHT, IMG_WIDTH), f\"Expected image shape ({labels.shape[0]}, 3, {IMG_HEIGHT}, {IMG_WIDTH}), but got {images.shape}\"\n",
        "assert images.dtype == torch.float32, f\"Expected image dtype torch.float32, but got {images.dtype}\"\n",
        "# After normalization, values are expected to be within a certain range, not necessarily [0, 1]\n",
        "# A simple check for non-NaN/Inf values is sufficient here.\n",
        "assert not torch.isnan(images).any(), \"Image batch contains NaN values\"\n",
        "assert not torch.isinf(images).any(), \"Image batch contains Inf values\"\n",
        "print(\"Image batch verification successful.\")\n",
        "\n",
        "\n",
        "# 6. Verify shape and range of values of the label tensor\n",
        "print(f\"Label batch shape: {labels.shape}\")\n",
        "print(f\"Label batch dtype: {labels.dtype}\")\n",
        "print(f\"Label batch min value: {torch.min(labels)}\")\n",
        "print(f\"Label batch max value: {torch.max(labels)}\")\n",
        "\n",
        "# Assert expected label properties\n",
        "assert labels.shape[0] == images.shape[0], f\"Expected label batch size {images.shape[0]}, but got {labels.shape[0]}\"\n",
        "assert labels.dtype == torch.int64, f\"Expected label dtype torch.int64, but got {labels.dtype}\"\n",
        "assert torch.min(labels) >= 0, f\"Expected minimum label value >= 0, but got {torch.min(labels)}\"\n",
        "assert torch.max(labels) < num_classes, f\"Expected maximum label value < {num_classes}, but got {torch.max(labels)}\"\n",
        "print(\"Label batch verification successful.\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of classes: 25\n",
            "Class names and indices: {'biryani': 0, 'burger': 1, 'chai': 2, 'chapati': 3, 'cholebhature': 4, 'dahl': 5, 'dhokla': 6, 'dosa': 7, 'friedrice': 8, 'gulabjamun': 9, 'idli': 10, 'jalebi': 11, 'kaathirolls': 12, 'kadaipaneer': 13, 'kulfi': 14, 'momos': 15, 'naan': 16, 'paanipuri': 17, 'pakode': 18, 'pavbhaji': 19, 'pizza': 20, 'poha': 21, 'rolls': 22, 'samosa': 23, 'vadapav': 24}\n",
            "Initialized dataset with 2613 images.\n",
            "Initialized dataset with 560 images.\n",
            "Initialized dataset with 560 images.\n",
            "DataLoaders created.\n",
            "\n",
            "Verifying first batch:\n",
            "Image batch shape: torch.Size([32, 3, 224, 224])\n",
            "Image batch dtype: torch.float32\n",
            "Image batch min value: -2.1179039478302\n",
            "Image batch max value: 2.640000104904175\n",
            "Image batch verification successful.\n",
            "Label batch shape: torch.Size([32])\n",
            "Label batch dtype: torch.int64\n",
            "Label batch min value: 0\n",
            "Label batch max value: 24\n",
            "Label batch verification successful.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94261d99"
      },
      "source": [
        "## Model definition\n",
        "\n",
        "### Subtask:\n",
        "Load a pretrained MobileNetV2 model, replace the classifier head, print the number of parameters, and verify the forward pass output shape.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7498d5b1"
      },
      "source": [
        "**Reasoning**:\n",
        "Load a pretrained MobileNetV2 model, replace the classifier head, print the number of parameters, create a dummy input, perform a forward pass, and verify the output shape.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aed3baf0",
        "outputId": "08287314-75db-4e84-98fd-40facb2f44d2"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "# 1. Import the mobilenet_v2 model from torchvision.models. (Done in previous steps implicitly by having access to torchvision)\n",
        "from torchvision.models import mobilenet_v2\n",
        "\n",
        "# 2. Load a pretrained MobileNetV2 model by calling mobilenet_v2 with pretrained=True.\n",
        "model = mobilenet_v2(pretrained=True)\n",
        "print(\"Pretrained MobileNetV2 loaded.\")\n",
        "\n",
        "# 3. Replace the last fully connected layer (classifier head) of the loaded model\n",
        "# with a new linear layer that has num_classes output features.\n",
        "# The classifier in MobileNetV2 is the last layer in the 'classifier' sequential module.\n",
        "num_ftrs = model.classifier[-1].in_features\n",
        "model.classifier[-1] = nn.Linear(num_ftrs, num_classes)\n",
        "print(f\"Classifier head replaced with a new linear layer with {num_classes} output features.\")\n",
        "\n",
        "# 4. Print the total number of trainable parameters in the modified model.\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total number of trainable parameters: {total_params}\")\n",
        "\n",
        "# 5. Create a dummy input tensor with the expected shape for a single image (e.g., (1, 3, IMG_HEIGHT, IMG_WIDTH)).\n",
        "dummy_input = torch.randn(1, 3, IMG_HEIGHT, IMG_WIDTH)\n",
        "print(f\"Created dummy input tensor with shape: {dummy_input.shape}\")\n",
        "\n",
        "# 6. Perform a forward pass through the modified model using the dummy input tensor.\n",
        "model.eval() # Set the model to evaluation mode\n",
        "with torch.no_grad(): # Disable gradient calculation for inference\n",
        "    output = model(dummy_input)\n",
        "print(\"Performed forward pass with dummy input.\")\n",
        "\n",
        "# 7. Print the shape of the output tensor from the forward pass.\n",
        "print(f\"Output tensor shape: {output.shape}\")\n",
        "\n",
        "# 8. Assert that the output shape is (1, num_classes) to verify the model's output dimensions.\n",
        "expected_output_shape = (1, num_classes)\n",
        "assert output.shape == expected_output_shape, f\"Expected output shape {expected_output_shape}, but got {output.shape}\"\n",
        "print(\"Output shape verification successful.\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\ProgramData\\miniconda3\\envs\\hack1\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "C:\\ProgramData\\miniconda3\\envs\\hack1\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to C:\\Users\\adria/.cache\\torch\\hub\\checkpoints\\mobilenet_v2-b0353104.pth\n",
            "100.0%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pretrained MobileNetV2 loaded.\n",
            "Classifier head replaced with a new linear layer with 25 output features.\n",
            "Total number of trainable parameters: 2255897\n",
            "Created dummy input tensor with shape: torch.Size([1, 3, 224, 224])\n",
            "Performed forward pass with dummy input.\n",
            "Output tensor shape: torch.Size([1, 25])\n",
            "Output shape verification successful.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "375bc94c"
      },
      "source": [
        "## Training\n",
        "\n",
        "### Subtask:\n",
        "Implement a training loop with mixed precision and gradient clipping. Train the classifier head first, then unfreeze and train the last blocks of the model. Log relevant metrics per epoch (loss, top-1/top-3 accuracy, per-class accuracy). Implement early stopping based on validation top-1 accuracy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbc65bed"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the training and validation functions, define the loss function and optimizers, move the model to the device, and set up the scaler for mixed precision training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "bdce8e36",
        "outputId": "fcd3b37c-569f-409e-cfde-b2d616ec69b0"
      },
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import copy\n",
        "from tqdm import tqdm\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "# 1. Move the model to the appropriate device (GPU if available, otherwise CPU).\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(f\"Model moved to: {device}\")\n",
        "\n",
        "# 2. Define the loss function (Cross-Entropy Loss) and optimizers for both training phases.\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_head_only = optim.Adam(model.classifier.parameters(), lr=HEAD_ONLY_LR)\n",
        "\n",
        "# Optimizer for fine-tuning will include parameters from the last few layers as well\n",
        "# We'll define this later after unfreezing layers\n",
        "\n",
        "# Setup GradScaler for mixed precision training\n",
        "scaler = GradScaler()\n",
        "print(\"Criterion, optimizer for head-only training, and GradScaler defined.\")\n",
        "\n",
        "# Helper function to calculate accuracy\n",
        "def calculate_accuracy(outputs, labels, topk=(1,)):\n",
        "    \"\"\"Calculates the top-k accuracy.\"\"\"\n",
        "    maxk = max(topk)\n",
        "    batch_size = labels.size(0)\n",
        "\n",
        "    _, pred = outputs.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(labels.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res\n",
        "\n",
        "# Helper function to calculate per-class accuracy\n",
        "def calculate_per_class_accuracy(outputs, labels, num_classes):\n",
        "    \"\"\"Calculates per-class accuracy.\"\"\"\n",
        "    _, preds = torch.max(outputs, 1)\n",
        "    correct = (preds == labels).squeeze()\n",
        "    class_correct = list(0. for i in range(num_classes))\n",
        "    class_total = list(0. for i in range(num_classes))\n",
        "    for i in range(labels.shape[0]):\n",
        "        label = labels[i]\n",
        "        class_correct[label] += correct[i].item()\n",
        "        class_total[label] += 1\n",
        "\n",
        "    per_class_acc = {}\n",
        "    for i in range(num_classes):\n",
        "        if class_total[i] > 0:\n",
        "            per_class_acc[i] = 100 * class_correct[i] / class_total[i]\n",
        "        else:\n",
        "            per_class_acc[i] = float('nan') # Handle classes not present in batch\n",
        "    return per_class_acc, class_total\n",
        "\n",
        "\n",
        "# 3. Implement the training function for one epoch\n",
        "def train_one_epoch(model, dataloader, criterion, optimizer, scaler, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "    all_labels = []\n",
        "    all_outputs = []\n",
        "\n",
        "    # Use tqdm for a progress bar\n",
        "    for images, labels in tqdm(dataloader, desc=\"Training\"):\n",
        "        if images is None or labels is None: # Skip batches with no valid images\n",
        "            continue\n",
        "\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Mixed precision training\n",
        "        with autocast():\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # Apply gradient clipping\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # Optional: Add gradient clipping if needed\n",
        "\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct_predictions += torch.sum(preds == labels.data)\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_outputs.extend(outputs.detach().cpu().numpy())\n",
        "\n",
        "\n",
        "    epoch_loss = running_loss / total_samples\n",
        "    epoch_acc = correct_predictions.double() / total_samples * 100\n",
        "\n",
        "    # Calculate top-1 and top-3 accuracy for the epoch\n",
        "    all_outputs_tensor = torch.tensor(all_outputs)\n",
        "    all_labels_tensor = torch.tensor(all_labels)\n",
        "    top1_acc, top3_acc = calculate_accuracy(all_outputs_tensor, all_labels_tensor, topk=(1, 3))\n",
        "\n",
        "    return epoch_loss, epoch_acc, top1_acc.item(), top3_acc.item()\n",
        "\n",
        "# 4. Implement the validation function for one epoch\n",
        "def validate_one_epoch(model, dataloader, criterion, device, num_classes):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "    all_labels = []\n",
        "    all_outputs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(dataloader, desc=\"Validation\"):\n",
        "            if images is None or labels is None: # Skip batches with no valid images\n",
        "                continue\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct_predictions += torch.sum(preds == labels.data)\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_outputs.extend(outputs.cpu().numpy())\n",
        "\n",
        "    epoch_loss = running_loss / total_samples\n",
        "    epoch_acc = correct_predictions.double() / total_samples * 100\n",
        "\n",
        "    # Calculate top-1 and top-3 accuracy for the epoch\n",
        "    all_outputs_tensor = torch.tensor(all_outputs)\n",
        "    all_labels_tensor = torch.tensor(all_labels)\n",
        "    top1_acc, top3_acc = calculate_accuracy(all_outputs_tensor, all_labels_tensor, topk=(1, 3))\n",
        "\n",
        "    # Calculate per-class accuracy\n",
        "    per_class_acc, class_total = calculate_per_class_accuracy(all_outputs_tensor, all_labels_tensor, num_classes)\n",
        "    per_class_acc_dict = {class_names[i]: acc for i, acc in per_class_acc.items()}\n",
        "\n",
        "    return epoch_loss, epoch_acc, top1_acc.item(), top3_acc.item(), per_class_acc_dict\n",
        "\n",
        "print(\"Training and validation functions defined.\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tqdm'",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[11], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcopy\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcuda\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mamp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GradScaler, autocast\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# 1. Move the model to the appropriate device (GPU if available, otherwise CPU).\u001b[39;00m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tqdm'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43e7309b"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because the `tqdm` library was not found. Install `tqdm` to resolve this issue and then retry the previous code block.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ay9-tWw2MVyT",
        "outputId": "026c26ee-669e-49f9-cd62-3359062b9189"
      },
      "source": [
        "%pip install tqdm"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tqdm\n",
            "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Requirement already satisfied: colorama in c:\\programdata\\miniconda3\\envs\\hack1\\lib\\site-packages (from tqdm) (0.4.6)\n",
            "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Installing collected packages: tqdm\n",
            "Successfully installed tqdm-4.67.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09f147a9"
      },
      "source": [
        "**Reasoning**:\n",
        "Retry the previous code block that failed due to the missing `tqdm` library. This block implements the training and validation functions and sets up the necessary components for training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f670c99",
        "outputId": "38ea4a7a-712f-4af5-ea61-924ebef42545"
      },
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import copy\n",
        "from tqdm import tqdm\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "# 1. Move the model to the appropriate device (GPU if available, otherwise CPU).\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(f\"Model moved to: {device}\")\n",
        "\n",
        "# 2. Define the loss function (Cross-Entropy Loss) and optimizers for both training phases.\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_head_only = optim.Adam(model.classifier.parameters(), lr=HEAD_ONLY_LR)\n",
        "\n",
        "# Optimizer for fine-tuning will include parameters from the last few layers as well\n",
        "# We'll define this later after unfreezing layers\n",
        "\n",
        "# Setup GradScaler for mixed precision training\n",
        "scaler = GradScaler()\n",
        "print(\"Criterion, optimizer for head-only training, and GradScaler defined.\")\n",
        "\n",
        "# Helper function to calculate accuracy\n",
        "def calculate_accuracy(outputs, labels, topk=(1,)):\n",
        "    \"\"\"Calculates the top-k accuracy.\"\"\"\n",
        "    maxk = max(topk)\n",
        "    batch_size = labels.size(0)\n",
        "\n",
        "    _, pred = outputs.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(labels.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res\n",
        "\n",
        "# Helper function to calculate per-class accuracy\n",
        "def calculate_per_class_accuracy(outputs, labels, num_classes):\n",
        "    \"\"\"Calculates per-class accuracy.\"\"\"\n",
        "    _, preds = torch.max(outputs, 1)\n",
        "    correct = (preds == labels).squeeze()\n",
        "    class_correct = list(0. for i in range(num_classes))\n",
        "    class_total = list(0. for i in range(num_classes))\n",
        "    for i in range(labels.shape[0]):\n",
        "        label = labels[i]\n",
        "        class_correct[label] += correct[i].item()\n",
        "        class_total[label] += 1\n",
        "\n",
        "    per_class_acc = {}\n",
        "    for i in range(num_classes):\n",
        "        if class_total[i] > 0:\n",
        "            per_class_acc[i] = 100 * class_correct[i] / class_total[i]\n",
        "        else:\n",
        "            per_class_acc[i] = float('nan') # Handle classes not present in batch\n",
        "    return per_class_acc, class_total\n",
        "\n",
        "\n",
        "# 3. Implement the training function for one epoch\n",
        "def train_one_epoch(model, dataloader, criterion, optimizer, scaler, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "    all_labels = []\n",
        "    all_outputs = []\n",
        "\n",
        "    # Use tqdm for a progress bar\n",
        "    for images, labels in tqdm(dataloader, desc=\"Training\"):\n",
        "        if images is None or labels is None: # Skip batches with no valid images\n",
        "            continue\n",
        "\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Mixed precision training\n",
        "        with autocast():\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # Apply gradient clipping\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # Optional: Add gradient clipping if needed\n",
        "\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct_predictions += torch.sum(preds == labels.data)\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_outputs.extend(outputs.detach().cpu().numpy())\n",
        "\n",
        "\n",
        "    epoch_loss = running_loss / total_samples\n",
        "    epoch_acc = correct_predictions.double() / total_samples * 100\n",
        "\n",
        "    # Calculate top-1 and top-3 accuracy for the epoch\n",
        "    all_outputs_tensor = torch.tensor(all_outputs)\n",
        "    all_labels_tensor = torch.tensor(all_labels)\n",
        "    top1_acc, top3_acc = calculate_accuracy(all_outputs_tensor, all_labels_tensor, topk=(1, 3))\n",
        "\n",
        "    return epoch_loss, epoch_acc, top1_acc.item(), top3_acc.item()\n",
        "\n",
        "# 4. Implement the validation function for one epoch\n",
        "def validate_one_epoch(model, dataloader, criterion, device, num_classes):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "    all_labels = []\n",
        "    all_outputs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(dataloader, desc=\"Validation\"):\n",
        "            if images is None or labels is None: # Skip batches with no valid images\n",
        "                continue\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct_predictions += torch.sum(preds == labels.data)\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_outputs.extend(outputs.cpu().numpy())\n",
        "\n",
        "    epoch_loss = running_loss / total_samples\n",
        "    epoch_acc = correct_predictions.double() / total_samples * 100\n",
        "\n",
        "    # Calculate top-1 and top-3 accuracy for the epoch\n",
        "    all_outputs_tensor = torch.tensor(all_outputs)\n",
        "    all_labels_tensor = torch.tensor(all_labels)\n",
        "    top1_acc, top3_acc = calculate_accuracy(all_outputs_tensor, all_labels_tensor, topk=(1, 3))\n",
        "\n",
        "    # Calculate per-class accuracy\n",
        "    per_class_acc, class_total = calculate_per_class_accuracy(all_outputs_tensor, all_labels_tensor, num_classes)\n",
        "    per_class_acc_dict = {class_names[i]: acc for i, acc in per_class_acc.items()}\n",
        "\n",
        "    return epoch_loss, epoch_acc, top1_acc.item(), top3_acc.item(), per_class_acc_dict\n",
        "\n",
        "print(\"Training and validation functions defined.\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model moved to: cuda\n",
            "Criterion, optimizer for head-only training, and GradScaler defined.\n",
            "Training and validation functions defined.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\adria\\AppData\\Local\\Temp\\ipykernel_66860\\1510231355.py:21: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a26c6df1"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the two-phase training process: head-only training followed by fine-tuning. This involves freezing/unfreezing layers, training loops, validation, logging metrics, and implementing early stopping.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udp4OL-vMgLX",
        "outputId": "517ef339-ab40-4319-a36a-8ad16a93d6e9"
      },
      "source": [
        "# 5. Train the classifier head only for HEAD_ONLY_EPOCHS:\n",
        "\n",
        "print(\"Starting head-only training...\")\n",
        "\n",
        "# Freeze all layers except the classifier head\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "for param in model.classifier.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "print(\"All layers except classifier head frozen.\")\n",
        "\n",
        "best_val_top1_acc = 0.0\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "early_stopping_patience = 5 # Number of epochs to wait for improvement\n",
        "epochs_no_improve = 0\n",
        "\n",
        "history = {\n",
        "    'head_only': {'train_loss': [], 'train_acc': [], 'train_top1_acc': [], 'train_top3_acc': [],\n",
        "                  'val_loss': [], 'val_acc': [], 'val_top1_acc': [], 'val_top3_acc': [], 'val_per_class_acc': []},\n",
        "    'fine_tune': {'train_loss': [], 'train_acc': [], 'train_top1_acc': [], 'train_top3_acc': [],\n",
        "                  'val_loss': [], 'val_acc': [], 'val_top1_acc': [], 'val_top3_acc': [], 'val_per_class_acc': []}\n",
        "}\n",
        "\n",
        "for epoch in range(HEAD_ONLY_EPOCHS):\n",
        "    epoch_start_time = time.time()\n",
        "    print(f\"\\nHead-Only Epoch {epoch+1}/{HEAD_ONLY_EPOCHS}\")\n",
        "    print('-' * 20)\n",
        "\n",
        "    # Training phase\n",
        "    train_loss, train_acc, train_top1_acc, train_top3_acc = train_one_epoch(\n",
        "        model, train_dataloader, criterion, optimizer_head_only, scaler, device\n",
        "    )\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Top-1 Acc: {train_top1_acc:.2f}%, Top-3 Acc: {train_top3_acc:.2f}%\")\n",
        "\n",
        "    # Validation phase\n",
        "    val_loss, val_acc, val_top1_acc, val_top3_acc, val_per_class_acc = validate_one_epoch(\n",
        "        model, val_dataloader, criterion, device, num_classes\n",
        "    )\n",
        "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%, Top-1 Acc: {val_top1_acc:.2f}%, Top-3 Acc: {val_top3_acc:.2f}%\")\n",
        "    # print(\"Per-Class Validation Accuracy:\", val_per_class_acc) # Optional: print per-class accuracy\n",
        "\n",
        "    # Log metrics\n",
        "    history['head_only']['train_loss'].append(train_loss)\n",
        "    history['head_only']['train_acc'].append(train_acc)\n",
        "    history['head_only']['train_top1_acc'].append(train_top1_acc)\n",
        "    history['head_only']['train_top3_acc'].append(train_top3_acc)\n",
        "    history['head_only']['val_loss'].append(val_loss)\n",
        "    history['head_only']['val_acc'].append(val_acc)\n",
        "    history['head_only']['val_top1_acc'].append(val_top1_acc)\n",
        "    history['head_only']['val_top3_acc'].append(val_top3_acc)\n",
        "    history['head_only']['val_per_class_acc'].append(val_per_class_acc) # Store the dict\n",
        "\n",
        "\n",
        "    # Check for early stopping based on validation top-1 accuracy\n",
        "    if val_top1_acc > best_val_top1_acc:\n",
        "        best_val_top1_acc = val_top1_acc\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        epochs_no_improve = 0\n",
        "        print(\"Validation Top-1 Accuracy improved. Saving best model checkpoint.\")\n",
        "        torch.save(model.state_dict(), os.path.join(CHECKPOINT_DIR, f'best_model_head_only_epoch_{epoch+1}.pth'))\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        print(f\"Validation Top-1 Accuracy did not improve for {epochs_no_improve} epoch(s).\")\n",
        "        if epochs_no_improve >= early_stopping_patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break # Stop training if patience is exceeded\n",
        "\n",
        "    epoch_end_time = time.time()\n",
        "    print(f\"Epoch {epoch+1} completed in {(epoch_end_time - epoch_start_time):.2f} seconds.\")\n",
        "\n",
        "\n",
        "print(\"\\nHead-only training finished.\")\n",
        "\n",
        "# 6. Unfreeze the last blocks of the model for fine-tuning:\n",
        "print(\"\\nStarting fine-tuning...\")\n",
        "\n",
        "# Load the best weights from head-only training\n",
        "model.load_state_dict(best_model_wts)\n",
        "print(\"Loaded best model weights from head-only training.\")\n",
        "\n",
        "# Unfreeze parameters in the last few layers/blocks of the feature extractor\n",
        "# MobileNetV2 features are in the 'features' sequential module.\n",
        "# We can unfreeze the last few blocks (e.g., the last few inverted residual blocks)\n",
        "# Let's unfreeze the last 3 blocks (indices 14, 15, 16) and the final convolution (index 17)\n",
        "# The classifier is already unfrozen\n",
        "unfreeze_layers_count = 4 # Unfreeze last 3 blocks + final conv\n",
        "for i, module in enumerate(reversed(model.features)):\n",
        "    if i < unfreeze_layers_count:\n",
        "        for param in module.parameters():\n",
        "            param.requires_grad = True\n",
        "        print(f\"Unfrozen features module: {len(model.features) - 1 - i}\")\n",
        "    else:\n",
        "        # Ensure earlier layers remain frozen\n",
        "        for param in module.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "\n",
        "# Define optimizer for fine-tuning (includes unfrozen feature layers and classifier)\n",
        "optimizer_fine_tune = optim.Adam([\n",
        "    {'params': filter(lambda p: p.requires_grad, model.features.parameters()), 'lr': FINE_TUNE_LR},\n",
        "    {'params': model.classifier.parameters(), 'lr': FINE_TUNE_LR} # Use the same learning rate for classifier during fine-tuning\n",
        "])\n",
        "\n",
        "print(\"Last feature blocks unfrozen and fine-tuning optimizer defined.\")\n",
        "\n",
        "# Reset early stopping parameters for fine-tuning\n",
        "best_val_top1_acc = history['head_only']['val_top1_acc'][-1] if history['head_only']['val_top1_acc'] else 0.0 # Start with the best acc from head-only\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "epochs_no_improve = 0\n",
        "\n",
        "# 7. Train the unfrozen layers and the classifier head for FINE_TUNE_EPOCHS:\n",
        "for epoch in range(FINE_TUNE_EPOCHS):\n",
        "    epoch_start_time = time.time()\n",
        "    print(f\"\\nFine-Tuning Epoch {epoch+1}/{FINE_TUNE_EPOCHS}\")\n",
        "    print('-' * 20)\n",
        "\n",
        "    # Training phase\n",
        "    train_loss, train_acc, train_top1_acc, train_top3_acc = train_one_epoch(\n",
        "        model, train_dataloader, criterion, optimizer_fine_tune, scaler, device\n",
        "    )\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Top-1 Acc: {train_top1_acc:.2f}%, Top-3 Acc: {train_top3_acc:.2f}%\")\n",
        "\n",
        "    # Validation phase\n",
        "    val_loss, val_acc, val_top1_acc, val_top3_acc, val_per_class_acc = validate_one_epoch(\n",
        "        model, val_dataloader, criterion, device, num_classes\n",
        "    )\n",
        "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%, Top-1 Acc: {val_top1_acc:.2f}%, Top-3 Acc: {val_top3_acc:.2f}%\")\n",
        "    # print(\"Per-Class Validation Accuracy:\", val_per_class_acc) # Optional: print per-class accuracy\n",
        "\n",
        "\n",
        "    # Log metrics\n",
        "    history['fine_tune']['train_loss'].append(train_loss)\n",
        "    history['fine_tune']['train_acc'].append(train_acc)\n",
        "    history['fine_tune']['train_top1_acc'].append(train_top1_acc)\n",
        "    history['fine_tune']['train_top3_acc'].append(train_top3_acc)\n",
        "    history['fine_tune']['val_loss'].append(val_loss)\n",
        "    history['fine_tune']['val_acc'].append(val_acc)\n",
        "    history['fine_tune']['val_top1_acc'].append(val_top1_acc)\n",
        "    history['fine_tune']['val_top3_acc'].append(val_top3_acc)\n",
        "    history['fine_tune']['val_per_class_acc'].append(val_per_class_acc) # Store the dict\n",
        "\n",
        "    # Check for early stopping based on validation top-1 accuracy\n",
        "    if val_top1_acc > best_val_top1_acc:\n",
        "        best_val_top1_acc = val_top1_acc\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        epochs_no_improve = 0\n",
        "        print(\"Validation Top-1 Accuracy improved. Saving best model checkpoint.\")\n",
        "        torch.save(model.state_dict(), os.path.join(CHECKPOINT_DIR, f'best_model_fine_tune_epoch_{epoch+1}.pth'))\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        print(f\"Validation Top-1 Accuracy did not improve for {epochs_no_improve} epoch(s).\")\n",
        "        if epochs_no_improve >= early_stopping_patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break # Stop training if patience is exceeded\n",
        "\n",
        "    epoch_end_time = time.time()\n",
        "    print(f\"Epoch {epoch+1} completed in {(epoch_end_time - epoch_start_time):.2f} seconds.\")\n",
        "\n",
        "print(\"\\nFine-tuning finished.\")\n",
        "\n",
        "# 8. Load the best model checkpoint after training.\n",
        "model.load_state_dict(best_model_wts)\n",
        "print(f\"\\nLoaded best model state from checkpoint with validation Top-1 accuracy: {best_val_top1_acc:.2f}%\")\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting head-only training...\n",
            "All layers except classifier head frozen.\n",
            "\n",
            "Head-Only Epoch 1/10\n",
            "--------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|                                                                                 | 0/82 [00:00<?, ?it/s]C:\\Users\\adria\\AppData\\Local\\Temp\\ipykernel_66860\\1510231355.py:80: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Training: 100%|| 82/82 [00:53<00:00,  1.52it/s]\n",
            "C:\\Users\\adria\\AppData\\Local\\Temp\\ipykernel_66860\\1510231355.py:105: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
            "  all_outputs_tensor = torch.tensor(all_outputs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 2.4707, Train Acc: 34.18%, Top-1 Acc: 34.18%, Top-3 Acc: 53.39%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 18/18 [00:10<00:00,  1.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 1.5050, Val Acc: 66.25%, Top-1 Acc: 66.25%, Top-3 Acc: 86.43%\n",
            "Validation Top-1 Accuracy improved. Saving best model checkpoint.\n",
            "Epoch 1 completed in 64.27 seconds.\n",
            "\n",
            "Head-Only Epoch 2/10\n",
            "--------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 82/82 [00:42<00:00,  1.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.5663, Train Acc: 59.89%, Top-1 Acc: 59.89%, Top-3 Acc: 80.06%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 18/18 [00:06<00:00,  2.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 1.0994, Val Acc: 71.43%, Top-1 Acc: 71.43%, Top-3 Acc: 88.21%\n",
            "Validation Top-1 Accuracy improved. Saving best model checkpoint.\n",
            "Epoch 2 completed in 49.36 seconds.\n",
            "\n",
            "Head-Only Epoch 3/10\n",
            "--------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 82/82 [00:41<00:00,  1.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.2890, Train Acc: 65.90%, Top-1 Acc: 65.90%, Top-3 Acc: 85.19%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 18/18 [00:06<00:00,  2.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.9127, Val Acc: 76.43%, Top-1 Acc: 76.43%, Top-3 Acc: 92.14%\n",
            "Validation Top-1 Accuracy improved. Saving best model checkpoint.\n",
            "Epoch 3 completed in 48.02 seconds.\n",
            "\n",
            "Head-Only Epoch 4/10\n",
            "--------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 82/82 [00:41<00:00,  1.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.1685, Train Acc: 67.93%, Top-1 Acc: 67.93%, Top-3 Acc: 86.34%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 18/18 [00:06<00:00,  2.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.8604, Val Acc: 75.36%, Top-1 Acc: 75.36%, Top-3 Acc: 91.96%\n",
            "Validation Top-1 Accuracy did not improve for 1 epoch(s).\n",
            "Epoch 4 completed in 48.18 seconds.\n",
            "\n",
            "Head-Only Epoch 5/10\n",
            "--------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 82/82 [00:41<00:00,  2.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.1107, Train Acc: 69.88%, Top-1 Acc: 69.88%, Top-3 Acc: 87.22%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 18/18 [00:06<00:00,  2.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.8270, Val Acc: 76.43%, Top-1 Acc: 76.43%, Top-3 Acc: 92.68%\n",
            "Validation Top-1 Accuracy did not improve for 2 epoch(s).\n",
            "Epoch 5 completed in 47.71 seconds.\n",
            "\n",
            "Head-Only Epoch 6/10\n",
            "--------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 82/82 [00:40<00:00,  2.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.0311, Train Acc: 70.91%, Top-1 Acc: 70.91%, Top-3 Acc: 88.48%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 18/18 [00:06<00:00,  2.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.7896, Val Acc: 75.89%, Top-1 Acc: 75.89%, Top-3 Acc: 92.32%\n",
            "Validation Top-1 Accuracy did not improve for 3 epoch(s).\n",
            "Epoch 6 completed in 47.32 seconds.\n",
            "\n",
            "Head-Only Epoch 7/10\n",
            "--------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 82/82 [00:40<00:00,  2.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9905, Train Acc: 71.34%, Top-1 Acc: 71.34%, Top-3 Acc: 88.17%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 18/18 [00:06<00:00,  2.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.7642, Val Acc: 77.50%, Top-1 Acc: 77.50%, Top-3 Acc: 93.21%\n",
            "Validation Top-1 Accuracy improved. Saving best model checkpoint.\n",
            "Epoch 7 completed in 47.47 seconds.\n",
            "\n",
            "Head-Only Epoch 8/10\n",
            "--------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 82/82 [00:40<00:00,  2.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9449, Train Acc: 73.52%, Top-1 Acc: 73.52%, Top-3 Acc: 88.90%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 18/18 [00:06<00:00,  2.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.7604, Val Acc: 79.46%, Top-1 Acc: 79.46%, Top-3 Acc: 93.57%\n",
            "Validation Top-1 Accuracy improved. Saving best model checkpoint.\n",
            "Epoch 8 completed in 47.30 seconds.\n",
            "\n",
            "Head-Only Epoch 9/10\n",
            "--------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 82/82 [00:40<00:00,  2.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9065, Train Acc: 74.51%, Top-1 Acc: 74.51%, Top-3 Acc: 89.67%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 18/18 [00:06<00:00,  2.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.7398, Val Acc: 77.50%, Top-1 Acc: 77.50%, Top-3 Acc: 93.04%\n",
            "Validation Top-1 Accuracy did not improve for 1 epoch(s).\n",
            "Epoch 9 completed in 47.58 seconds.\n",
            "\n",
            "Head-Only Epoch 10/10\n",
            "--------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 82/82 [00:39<00:00,  2.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9288, Train Acc: 73.13%, Top-1 Acc: 73.13%, Top-3 Acc: 89.55%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 18/18 [00:06<00:00,  2.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.7598, Val Acc: 78.75%, Top-1 Acc: 78.75%, Top-3 Acc: 92.32%\n",
            "Validation Top-1 Accuracy did not improve for 2 epoch(s).\n",
            "Epoch 10 completed in 46.56 seconds.\n",
            "\n",
            "Head-only training finished.\n",
            "\n",
            "Starting fine-tuning...\n",
            "Loaded best model weights from head-only training.\n",
            "Unfrozen features module: 18\n",
            "Unfrozen features module: 17\n",
            "Unfrozen features module: 16\n",
            "Unfrozen features module: 15\n",
            "Last feature blocks unfrozen and fine-tuning optimizer defined.\n",
            "\n",
            "Fine-Tuning Epoch 1/20\n",
            "--------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 82/82 [00:40<00:00,  2.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.8639, Train Acc: 75.43%, Top-1 Acc: 75.43%, Top-3 Acc: 89.44%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 18/18 [00:06<00:00,  2.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.6591, Val Acc: 80.54%, Top-1 Acc: 80.54%, Top-3 Acc: 94.11%\n",
            "Validation Top-1 Accuracy improved. Saving best model checkpoint.\n",
            "Epoch 1 completed in 47.17 seconds.\n",
            "\n",
            "Fine-Tuning Epoch 2/20\n",
            "--------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 82/82 [00:39<00:00,  2.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.7099, Train Acc: 78.99%, Top-1 Acc: 78.99%, Top-3 Acc: 92.19%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 18/18 [00:06<00:00,  2.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.6194, Val Acc: 79.46%, Top-1 Acc: 79.46%, Top-3 Acc: 94.64%\n",
            "Validation Top-1 Accuracy did not improve for 1 epoch(s).\n",
            "Epoch 2 completed in 46.54 seconds.\n",
            "\n",
            "Fine-Tuning Epoch 3/20\n",
            "--------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 82/82 [00:39<00:00,  2.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5981, Train Acc: 82.28%, Top-1 Acc: 82.28%, Top-3 Acc: 94.03%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 18/18 [00:06<00:00,  2.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.5831, Val Acc: 81.43%, Top-1 Acc: 81.43%, Top-3 Acc: 95.18%\n",
            "Validation Top-1 Accuracy improved. Saving best model checkpoint.\n",
            "Epoch 3 completed in 46.81 seconds.\n",
            "\n",
            "Fine-Tuning Epoch 4/20\n",
            "--------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 82/82 [00:40<00:00,  2.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5872, Train Acc: 82.47%, Top-1 Acc: 82.47%, Top-3 Acc: 93.99%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 18/18 [00:06<00:00,  2.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.5635, Val Acc: 83.04%, Top-1 Acc: 83.04%, Top-3 Acc: 94.64%\n",
            "Validation Top-1 Accuracy improved. Saving best model checkpoint.\n",
            "Epoch 4 completed in 46.99 seconds.\n",
            "\n",
            "Fine-Tuning Epoch 5/20\n",
            "--------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 82/82 [00:40<00:00,  2.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5353, Train Acc: 83.74%, Top-1 Acc: 83.74%, Top-3 Acc: 94.34%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 18/18 [00:06<00:00,  2.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.5344, Val Acc: 83.75%, Top-1 Acc: 83.75%, Top-3 Acc: 96.07%\n",
            "Validation Top-1 Accuracy improved. Saving best model checkpoint.\n",
            "Epoch 5 completed in 47.38 seconds.\n",
            "\n",
            "Fine-Tuning Epoch 6/20\n",
            "--------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 82/82 [00:39<00:00,  2.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5187, Train Acc: 84.92%, Top-1 Acc: 84.92%, Top-3 Acc: 95.56%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 18/18 [00:06<00:00,  2.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.5771, Val Acc: 82.50%, Top-1 Acc: 82.50%, Top-3 Acc: 95.18%\n",
            "Validation Top-1 Accuracy did not improve for 1 epoch(s).\n",
            "Epoch 6 completed in 46.47 seconds.\n",
            "\n",
            "Fine-Tuning Epoch 7/20\n",
            "--------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 82/82 [00:40<00:00,  2.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.4357, Train Acc: 87.33%, Top-1 Acc: 87.33%, Top-3 Acc: 96.52%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 18/18 [00:06<00:00,  2.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.5607, Val Acc: 82.50%, Top-1 Acc: 82.50%, Top-3 Acc: 95.18%\n",
            "Validation Top-1 Accuracy did not improve for 2 epoch(s).\n",
            "Epoch 7 completed in 47.57 seconds.\n",
            "\n",
            "Fine-Tuning Epoch 8/20\n",
            "--------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 82/82 [00:39<00:00,  2.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.4370, Train Acc: 87.22%, Top-1 Acc: 87.22%, Top-3 Acc: 96.17%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 18/18 [00:06<00:00,  2.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.5561, Val Acc: 83.75%, Top-1 Acc: 83.75%, Top-3 Acc: 94.64%\n",
            "Validation Top-1 Accuracy did not improve for 3 epoch(s).\n",
            "Epoch 8 completed in 46.79 seconds.\n",
            "\n",
            "Fine-Tuning Epoch 9/20\n",
            "--------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 82/82 [00:40<00:00,  2.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.4416, Train Acc: 87.03%, Top-1 Acc: 87.03%, Top-3 Acc: 96.06%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 18/18 [00:06<00:00,  2.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.5834, Val Acc: 83.39%, Top-1 Acc: 83.39%, Top-3 Acc: 93.04%\n",
            "Validation Top-1 Accuracy did not improve for 4 epoch(s).\n",
            "Epoch 9 completed in 47.24 seconds.\n",
            "\n",
            "Fine-Tuning Epoch 10/20\n",
            "--------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 82/82 [00:40<00:00,  2.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.4027, Train Acc: 88.90%, Top-1 Acc: 88.90%, Top-3 Acc: 96.79%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|| 18/18 [00:06<00:00,  2.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.5839, Val Acc: 83.04%, Top-1 Acc: 83.04%, Top-3 Acc: 94.82%\n",
            "Validation Top-1 Accuracy did not improve for 5 epoch(s).\n",
            "Early stopping triggered.\n",
            "\n",
            "Fine-tuning finished.\n",
            "\n",
            "Loaded best model state from checkpoint with validation Top-1 accuracy: 83.75%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5b69f16"
      },
      "source": [
        "**Reasoning**:\n",
        "Plot the training and validation loss and accuracy curves for both training phases to visualize the model's performance over epochs. This helps in understanding the training progress and identifying potential overfitting.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "ingq8guYQO3M",
        "outputId": "f68ba02c-0359-4fac-9c29-2c2137c9157a"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting the training history\n",
        "def plot_history(history):\n",
        "    epochs_head_only = range(1, len(history['head_only']['train_loss']) + 1)\n",
        "    epochs_fine_tune = range(1, len(history['fine_tune']['train_loss']) + 1)\n",
        "    epochs_fine_tune_start = len(history['head_only']['train_loss'])\n",
        "\n",
        "    # Plot Loss\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs_head_only, history['head_only']['train_loss'], label='Head Only Train Loss')\n",
        "    plt.plot(epochs_head_only, history['head_only']['val_loss'], label='Head Only Val Loss')\n",
        "    if epochs_fine_tune:\n",
        "        plt.plot(range(epochs_fine_tune_start + 1, epochs_fine_tune_start + len(epochs_fine_tune) + 1), history['fine_tune']['train_loss'], label='Fine Tune Train Loss')\n",
        "        plt.plot(range(epochs_fine_tune_start + 1, epochs_fine_tune_start + len(epochs_fine_tune) + 1), history['fine_tune']['val_loss'], label='Fine Tune Val Loss')\n",
        "\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot Accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs_head_only, history['head_only']['train_top1_acc'], label='Head Only Train Top-1 Acc')\n",
        "    plt.plot(epochs_head_only, history['head_only']['val_top1_acc'], label='Head Only Val Top-1 Acc')\n",
        "    if epochs_fine_tune:\n",
        "        plt.plot(range(epochs_fine_tune_start + 1, epochs_fine_tune_start + len(epochs_fine_tune) + 1), history['fine_tune']['train_top1_acc'], label='Fine Tune Train Top-1 Acc')\n",
        "        plt.plot(range(epochs_fine_tune_start + 1, epochs_fine_tune_start + len(epochs_fine_tune) + 1), history['fine_tune']['val_top1_acc'], label='Fine Tune Val Top-1 Acc')\n",
        "\n",
        "    plt.title('Training and Validation Top-1 Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot the history\n",
        "plot_history(history)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 2 Axes>"
            ],
            "image/png": "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"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e368ff6"
      },
      "source": [
        "**Reasoning**:\n",
        "Evaluate the performance of the best model on the test dataset using Top-1 and Top-3 accuracy. This provides a final assessment of the model's generalization ability on unseen data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b35a2a34",
        "outputId": "5aec528f-baa1-4a3b-abc9-623e716e736a"
      },
      "source": [
        "# 9. Evaluate the best model on the test dataset:\n",
        "print(\"\\nEvaluating the best model on the test dataset...\")\n",
        "\n",
        "# Put the model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "test_loss = 0.0\n",
        "test_correct_top1 = 0\n",
        "test_correct_top3 = 0\n",
        "test_total = 0\n",
        "all_test_labels = []\n",
        "all_test_outputs = []\n",
        "\n",
        "# Disable gradient calculation for evaluation\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_dataloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        test_loss += loss.item() * inputs.size(0)\n",
        "        test_total += labels.size(0)\n",
        "\n",
        "        # Calculate Top-1 and Top-3 accuracy\n",
        "        _, predicted_top1 = outputs.topk(1, 1, True, True)\n",
        "        test_correct_top1 += torch.sum(predicted_top1.squeeze() == labels).item()\n",
        "\n",
        "        _, predicted_top3 = outputs.topk(3, 1, True, True)\n",
        "        test_correct_top3 += torch.sum(predicted_top3.T == labels).item()\n",
        "\n",
        "        all_test_labels.extend(labels.cpu().numpy())\n",
        "        all_test_outputs.extend(outputs.cpu().numpy())\n",
        "\n",
        "\n",
        "test_loss /= test_total\n",
        "test_top1_acc = (test_correct_top1 / test_total) * 100\n",
        "test_top3_acc = (test_correct_top3 / test_total) * 100\n",
        "\n",
        "print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Top-1 Accuracy: {test_top1_acc:.2f}%\")\n",
        "print(f\"Test Top-3 Accuracy: {test_top3_acc:.2f}%\")\n",
        "\n",
        "# Calculate per-class accuracy on the test set\n",
        "per_class_test_correct = {i: 0 for i in range(num_classes)}\n",
        "per_class_test_total = {i: 0 for i in range(num_classes)}\n",
        "\n",
        "all_test_outputs_tensor = torch.tensor(all_test_outputs)\n",
        "all_test_labels_tensor = torch.tensor(all_test_labels)\n",
        "\n",
        "_, predicted_test = all_test_outputs_tensor.topk(1, 1, True, True)\n",
        "predicted_test = predicted_test.squeeze()\n",
        "\n",
        "for i in range(len(all_test_labels_tensor)):\n",
        "    label = all_test_labels_tensor[i].item()\n",
        "    per_class_test_total[label] += 1\n",
        "    if predicted_test[i].item() == label:\n",
        "        per_class_test_correct[label] += 1\n",
        "\n",
        "per_class_test_acc = {class_names[i]: (per_class_test_correct[i] / per_class_test_total[i]) * 100 if per_class_test_total[i] > 0 else 0 for i in range(num_classes)}\n",
        "\n",
        "print(\"\\nPer-Class Test Accuracy:\")\n",
        "for class_name, accuracy in per_class_test_acc.items():\n",
        "    print(f\"{class_name}: {accuracy:.2f}%\")\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating the best model on the test dataset...\n",
            "\n",
            "Test Loss: 0.5900\n",
            "Test Top-1 Accuracy: 83.39%\n",
            "Test Top-3 Accuracy: 93.75%\n",
            "\n",
            "Per-Class Test Accuracy:\n",
            "biryani: 77.27%\n",
            "burger: 95.45%\n",
            "chai: 100.00%\n",
            "chapati: 82.61%\n",
            "cholebhature: 91.30%\n",
            "dahl: 95.45%\n",
            "dhokla: 73.91%\n",
            "dosa: 86.96%\n",
            "friedrice: 72.73%\n",
            "gulabjamun: 95.45%\n",
            "idli: 91.30%\n",
            "jalebi: 81.82%\n",
            "kaathirolls: 59.09%\n",
            "kadaipaneer: 59.09%\n",
            "kulfi: 60.00%\n",
            "momos: 82.61%\n",
            "naan: 78.26%\n",
            "paanipuri: 86.36%\n",
            "pakode: 86.96%\n",
            "pavbhaji: 86.36%\n",
            "pizza: 82.61%\n",
            "poha: 91.30%\n",
            "rolls: 81.82%\n",
            "samosa: 82.61%\n",
            "vadapav: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "75psoFRfSD5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ec85f532",
        "outputId": "d7efa297-762c-4610-ccc5-f226b9be3286"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import torch\n",
        "import onnx\n",
        "import onnxruntime\n",
        "import numpy as np\n",
        "\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "# ======================\n",
        "# Preconditions and inputs\n",
        "# ======================\n",
        "required_vars = [\n",
        "    'model', 'class_to_idx', 'num_classes',\n",
        "    'device', 'IMG_HEIGHT', 'IMG_WIDTH',\n",
        "    'test_loss', 'test_top1_acc', 'test_top3_acc', 'per_class_test_acc'\n",
        "]\n",
        "missing = [v for v in required_vars if v not in globals()]\n",
        "if missing:\n",
        "    raise ValueError(f\"Missing required variables in the current session: {missing}. \"\n",
        "                     f\"Ensure training defined these before running export.\")\n",
        "\n",
        "if not isinstance(class_to_idx, dict) or not isinstance(num_classes, int):\n",
        "    raise ValueError(\"class_to_idx must be a dict and num_classes must be an int.\")\n",
        "\n",
        "if len(class_to_idx) != num_classes:\n",
        "    raise ValueError(f\"Mismatch: len(class_to_idx)={len(class_to_idx)} vs num_classes={num_classes}.\")\n",
        "\n",
        "if not isinstance(IMG_HEIGHT, int) or not isinstance(IMG_WIDTH, int):\n",
        "    raise ValueError(\"IMG_HEIGHT and IMG_WIDTH must be integers, e.g., 224.\")\n",
        "\n",
        "# ======================\n",
        "# 0) Save directory\n",
        "# ======================\n",
        "SAVE_DIR = r\"C:\\Users\\adria\\OneDrive\\Desktop\\CODING\\simplyyou\\mobilenet\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "print(f\"Artifacts will be saved to: {SAVE_DIR}\")\n",
        "\n",
        "# ======================\n",
        "# 1) Save best checkpoint (.pth)\n",
        "# ======================\n",
        "final_best_model_path = os.path.join(SAVE_DIR, \"best_mobilenetv2.pth\")\n",
        "# Ensure eval mode before saving (ok either way, but consistent)\n",
        "model.eval()\n",
        "torch.save(model.state_dict(), final_best_model_path)\n",
        "print(f\"Best model saved to: {final_best_model_path}\")\n",
        "\n",
        "# ======================\n",
        "# 2) Save label map JSON\n",
        "# ======================\n",
        "label_map_path = os.path.join(SAVE_DIR, \"label_map.json\")\n",
        "with open(label_map_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(class_to_idx, f, ensure_ascii=False, indent=2)\n",
        "print(f\"Label map saved to: {label_map_path}\")\n",
        "\n",
        "# ======================\n",
        "# 3) Export ONNX with dynamic batch\n",
        "# ======================\n",
        "onnx_model_path = os.path.join(SAVE_DIR, \"mobilenetv2_224.onnx\")\n",
        "model.eval()\n",
        "\n",
        "# Create a dummy input on the same device for export, but ORT will receive numpy CPU\n",
        "dummy_input_onnx = torch.randn(1, 3, IMG_HEIGHT, IMG_WIDTH, device=device)\n",
        "\n",
        "try:\n",
        "    torch.onnx.export(\n",
        "        model,\n",
        "        dummy_input_onnx,\n",
        "        onnx_model_path,\n",
        "        export_params=True,\n",
        "        opset_version=12,  # slightly newer than 11; adjust if environment requires\n",
        "        do_constant_folding=True,\n",
        "        input_names=[\"input\"],\n",
        "        output_names=[\"output\"],\n",
        "        dynamic_axes={\n",
        "            \"input\": {0: \"batch_size\"},\n",
        "            \"output\": {0: \"batch_size\"}\n",
        "        }\n",
        "    )\n",
        "    print(f\"Model exported to ONNX format at: {onnx_model_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"ONNX export failed: {e}\")\n",
        "    print(\"Hint: If opset_version 12 fails, try 11. Ensure the model forward uses operations supported by ONNX.\")\n",
        "    raise\n",
        "\n",
        "# ======================\n",
        "# 4) Verify ONNX\n",
        "# ======================\n",
        "try:\n",
        "    onnx.checker.check_model(onnx_model_path)\n",
        "    print(\"ONNX model is valid.\")\n",
        "except onnx.checker.ValidationError as e:\n",
        "    print(f\"ONNX model validation failed: {e}\")\n",
        "    raise\n",
        "\n",
        "# Verify ONNX Runtime session and I/O shapes\n",
        "try:\n",
        "    # Use CPU provider to avoid GPU provider issues if not installed\n",
        "    ort_sess = onnxruntime.InferenceSession(\n",
        "        onnx_model_path,\n",
        "        providers=[\"CPUExecutionProvider\"]\n",
        "    )\n",
        "    print(\"ONNX Runtime session created (CPU provider).\")\n",
        "\n",
        "    input_name = ort_sess.get_inputs()[0].name\n",
        "    output_name = ort_sess.get_outputs()[0].name\n",
        "    print(f\"ONNX IO names -> input: {input_name}, output: {output_name}\")\n",
        "\n",
        "    input_shape = ort_sess.get_inputs()[0].shape  # e.g., ['batch_size', 3, 224, 224] or [None, 3, 224, 224]\n",
        "    output_shape = ort_sess.get_outputs()[0].shape  # e.g., ['batch_size', num_classes]\n",
        "    print(f\"ONNX Input shape: {input_shape}\")\n",
        "    print(f\"ONNX Output shape: {output_shape}\")\n",
        "\n",
        "    # Helper to interpret ONNX dynamic dim\n",
        "    def is_dynamic(dim):\n",
        "        return dim is None or isinstance(dim, str)\n",
        "\n",
        "    # Input checks\n",
        "    assert is_dynamic(input_shape[0]), \"ONNX input batch dimension is not dynamic.\"\n",
        "    assert input_shape[1] == 3, f\"ONNX input channels expected 3, got {input_shape[1]}\"\n",
        "    assert input_shape[2] == IMG_HEIGHT and input_shape[3] == IMG_WIDTH, \\\n",
        "        f\"ONNX input HxW expected {IMG_HEIGHT}x{IMG_WIDTH}, got {input_shape[2]}x{input_shape[3]}\"\n",
        "    print(\"ONNX input shape verification successful.\")\n",
        "\n",
        "    # Output checks\n",
        "    assert is_dynamic(output_shape[0]), \"ONNX output batch dimension is not dynamic.\"\n",
        "    assert output_shape[1] == num_classes, \\\n",
        "        f\"ONNX output classes mismatch: expected {num_classes}, got {output_shape[1]}\"\n",
        "    print(\"ONNX output shape verification successful.\")\n",
        "\n",
        "    # Test inference with CPU numpy\n",
        "    ort_inputs = {input_name: dummy_input_onnx.detach().cpu().numpy()}\n",
        "    ort_outputs = ort_sess.run([output_name], ort_inputs)\n",
        "    print(\"ONNX Runtime inference successful with dummy input.\")\n",
        "    print(f\"ONNX Runtime output shape: {ort_outputs[0].shape}\")\n",
        "except Exception as e:\n",
        "    print(f\"ONNX Runtime verification failed: {e}\")\n",
        "    raise\n",
        "\n",
        "# ======================\n",
        "# 5) Generate infer_one.py\n",
        "# ======================\n",
        "# Embed label map safely as JSON string literal\n",
        "label_map_json_string = json.dumps(class_to_idx).replace(\"\\\\\", \"\\\\\\\\\").replace(\"'\", \"\\\\'\")\n",
        "\n",
        "inference_script_content = f\"\"\"import os\n",
        "import sys\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "import onnxruntime\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "IMG_HEIGHT = {IMG_HEIGHT}\n",
        "IMG_WIDTH = {IMG_WIDTH}\n",
        "IMG_SIZE = (IMG_HEIGHT, IMG_WIDTH)\n",
        "\n",
        "# Embedded label map (index mapping)\n",
        "label_map = json.loads('{label_map_json_string}')\n",
        "num_classes_loaded = len(label_map)\n",
        "\n",
        "def load_model(model_path, num_classes, device):\n",
        "    from torchvision.models import mobilenet_v2\n",
        "    model = mobilenet_v2(weights=None)  # pretrained=False (new API uses weights=None)\n",
        "    num_ftrs = model.classifier[-1].in_features\n",
        "    model.classifier[-1] = torch.nn.Linear(num_ftrs, num_classes)\n",
        "    state = torch.load(model_path, map_location=device)\n",
        "    model.load_state_dict(state)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "def load_onnx(model_path):\n",
        "    sess = onnxruntime.InferenceSession(model_path, providers=[\"CPUExecutionProvider\"])\n",
        "    return sess\n",
        "\n",
        "def preprocess(image_path):\n",
        "    tfm = transforms.Compose([\n",
        "        transforms.Resize((IMG_HEIGHT, IMG_WIDTH)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    img = Image.open(image_path).convert('RGB')\n",
        "    t = tfm(img).unsqueeze(0)  # [1,3,H,W]\n",
        "    return t\n",
        "\n",
        "def infer_torch(model, image_tensor, label_map):\n",
        "    with torch.no_grad():\n",
        "        logits = model(image_tensor)\n",
        "    probs = torch.softmax(logits[0], dim=0)\n",
        "    top5_prob, top5_idx = torch.topk(probs, 5)\n",
        "    idx_to_class = {{str(v): k for k, v in label_map.items()}}\n",
        "    print(\"Top 5 predictions (PyTorch):\")\n",
        "    for p, i in zip(top5_prob.tolist(), top5_idx.tolist()):\n",
        "        print(f\"  {{idx_to_class.get(str(i), i)}}: {{p:.4f}}\")\n",
        "\n",
        "def infer_onnx(sess, image_tensor, label_map):\n",
        "    inp_name = sess.get_inputs()[0].name\n",
        "    out_name = sess.get_outputs()[0].name\n",
        "    ort_in = {{inp_name: image_tensor.detach().cpu().numpy()}}\n",
        "    out = sess.run([out_name], ort_in)[0]  # [1,num_classes]\n",
        "    import torch as _torch\n",
        "    probs = _torch.softmax(_torch.tensor(out[0]), dim=0)\n",
        "    top5_prob, top5_idx = _torch.topk(probs, 5)\n",
        "    idx_to_class = {{str(v): k for k, v in label_map.items()}}\n",
        "    print(\"Top 5 predictions (ONNX):\")\n",
        "    for p, i in zip(top5_prob.tolist(), top5_idx.tolist()):\n",
        "        print(f\"  {{idx_to_class.get(str(i), i)}}: {{p:.4f}}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if len(sys.argv) != 3:\n",
        "        print(\"Usage: python infer_one.py <model_path(.pth|.onnx)> <image_path>\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    model_path = sys.argv[1]\n",
        "    image_path = sys.argv[2]\n",
        "\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\"Model file not found: {{model_path}}\")\n",
        "        sys.exit(1)\n",
        "    if not os.path.exists(image_path):\n",
        "        print(f\"Image file not found: {{image_path}}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # Preprocess\n",
        "    try:\n",
        "        image_tensor = preprocess(image_path)\n",
        "        print(f\"Preprocessed image tensor: {{tuple(image_tensor.shape)}} (expect (1,3,{IMG_HEIGHT},{IMG_WIDTH}))\")\n",
        "    except Exception as e:\n",
        "        print(f\"Preprocess failed: {{e}}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # Run\n",
        "    if model_path.lower().endswith(\".pth\"):\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        try:\n",
        "            model = load_model(model_path, num_classes_loaded, device)\n",
        "            infer_torch(model, image_tensor.to(device), label_map)\n",
        "        except Exception as e:\n",
        "            print(f\"PyTorch inference failed: {{e}}\")\n",
        "            sys.exit(1)\n",
        "\n",
        "    elif model_path.lower().endswith(\".onnx\"):\n",
        "        try:\n",
        "            sess = load_onnx(model_path)\n",
        "            infer_onnx(sess, image_tensor, label_map)\n",
        "        except Exception as e:\n",
        "            print(f\"ONNX inference failed: {{e}}\")\n",
        "            sys.exit(1)\n",
        "    else:\n",
        "        print(\"Unsupported model extension. Use .pth or .onnx\")\n",
        "        sys.exit(1)\n",
        "\"\"\"\n",
        "\n",
        "inference_script_path_full = os.path.join(SAVE_DIR, \"infer_one.py\")\n",
        "with open(inference_script_path_full, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(inference_script_content)\n",
        "print(f\"Inference script saved to: {inference_script_path_full}\")\n",
        "\n",
        "# ======================\n",
        "# 6) README.md\n",
        "# ======================\n",
        "readme_path = os.path.join(SAVE_DIR, \"README.md\")\n",
        "readme_content = f\"\"\"# Food Image Classification Model\n",
        "\n",
        "This folder contains a fine-tuned MobileNetV2 model for Indian food classification.\n",
        "\n",
        "## Models\n",
        "- PyTorch: best_mobilenetv2.pth\n",
        "- ONNX: mobilenetv2_224.onnx\n",
        "- Labels: label_map.json (index mapping used during training)\n",
        "\n",
        "## Test Metrics\n",
        "- Test Loss: {test_loss:.4f}\n",
        "- Test Top-1 Accuracy: {test_top1_acc:.2f}%\n",
        "- Test Top-3 Accuracy: {test_top3_acc:.2f}%\n",
        "\n",
        "### Per-Class Accuracy\n",
        "\"\"\"  # per-class list appended next\n",
        "\n",
        "for class_name, acc in per_class_test_acc.items():\n",
        "    readme_content += f\"- {class_name}: {acc:.2f}%\\n\"\n",
        "\n",
        "readme_content += f\"\"\"\n",
        "## Inference\n",
        "\n",
        "### Requirements\n",
        "- Python 3.9+\n",
        "- pip install torch torchvision onnx onnxruntime pillow numpy\n",
        "\n",
        "### Run\n",
        "- PyTorch:\n",
        "  python \"{inference_script_path_full}\" \"{final_best_model_path}\" \"C:\\\\path\\\\to\\\\image.jpg\"\n",
        "- ONNX:\n",
        "  python \"{inference_script_path_full}\" \"{onnx_model_path}\" \"C:\\\\path\\\\to\\\\image.jpg\"\n",
        "\n",
        "Notes:\n",
        "- Input is resized to {IMG_HEIGHT}x{IMG_WIDTH} with ImageNet normalization.\n",
        "- For ONNX, CPUExecutionProvider is used by default; add CUDA provider if configured.\n",
        "\"\"\"\n",
        "\n",
        "with open(readme_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(readme_content)\n",
        "print(f\"README.md saved to: {readme_path}\")\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Artifacts will be saved to: C:\\Users\\adria\\OneDrive\\Desktop\\CODING\\simplyyou\\mobilenet\n",
            "Best model saved to: C:\\Users\\adria\\OneDrive\\Desktop\\CODING\\simplyyou\\mobilenet\\best_mobilenetv2.pth\n",
            "Label map saved to: C:\\Users\\adria\\OneDrive\\Desktop\\CODING\\simplyyou\\mobilenet\\label_map.json\n",
            "Model exported to ONNX format at: C:\\Users\\adria\\OneDrive\\Desktop\\CODING\\simplyyou\\mobilenet\\mobilenetv2_224.onnx\n",
            "ONNX model is valid.\n",
            "ONNX Runtime session created (CPU provider).\n",
            "ONNX IO names -> input: input, output: output\n",
            "ONNX Input shape: ['batch_size', 3, 224, 224]\n",
            "ONNX Output shape: ['batch_size', 25]\n",
            "ONNX input shape verification successful.\n",
            "ONNX output shape verification successful.\n",
            "ONNX Runtime inference successful with dummy input.\n",
            "ONNX Runtime output shape: (1, 25)\n",
            "Inference script saved to: C:\\Users\\adria\\OneDrive\\Desktop\\CODING\\simplyyou\\mobilenet\\infer_one.py\n",
            "README.md saved to: C:\\Users\\adria\\OneDrive\\Desktop\\CODING\\simplyyou\\mobilenet\\README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f5mf6ktGTMGV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}